https://arxiv.org/abs/2509.09677

*The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs* (Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping)

> Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.

인공적인 과제를 사용해서 모델이 수행할 수 있는 과제의 길이에 대해 분석. 1. 모델 크기 증가는 한 스텝에 대한 성능에는 이득이 감소하지만 수행할 수 있는 과제의 길이를 증가시킨다. 2. 모델이 자신의 생성 결과에 (오류) 의존하는 Self-conditioning은 모델 규모 증가에 따라 증가한다. 모델이 자신의 생성 결과를 증거로 간주하는 고전적인 문제 (https://arxiv.org/abs/2110.10819). 3. 그렇지만 추론이 Self-conditioning을 해결한다.

Analysis on the length of tasks models can complete, using synthetic tasks. 1. Scaling models has diminishing returns in single-step accuracy, but it increases the length of tasks they can successfully complete. 2. Self-conditioning of models on their own generation (errors) worsens as model scale increases. This is a classical problem models considering their own generation as evidence (https://arxiv.org/abs/2110.10819). 3. However, reasoning solves this self-conditioning.

#agent 