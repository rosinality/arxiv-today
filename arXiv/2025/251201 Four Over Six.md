https://arxiv.org/abs/2512.02010

*Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling* (Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han)

> As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.

NVFP4 양자화에 대해서는 값의 범위를 (-6, 6)이 아니라 (-4, 4)로 설정하는 것이 나을 수 있음. 그런데 왜 NVFP4 학습 베이스라인이 모두 발산하는 것일지?

It could be better to scale value ranges to (-4, 4) instead of (-6, 6) for NVFP4 quantization. But why do all NVFP4 training baselines diverge?

#quantization 