https://arxiv.org/abs/2511.11373

*MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism* (Shulin Liu, Dong Du, Tao Yang, Yang Li, Boyu Qiu)

> Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

멀티 에이전트 RL 프레임워크. 에이전트가 다른 에이전트의 결과와는 독립적으로 보상을 얻을 수 있다는 것을 이용해 개별적으로 학습됨. 다른 에이전트의 결과가 보상 부여에 필요한 경우에도 있기에 그런 경우에는 프레임워크에서 의존성 그래프를 다루는 것이 필요해질지도?

Multi-agent RL framework. In this case each agent gets reward independently from other agents' results. Thus it can be trained in a decoupled manner. Though there would be cases where the reward for each agent relies on other agents' results, and to support this maybe frameworks that utilize a dependency graph would be needed?

#framework #agent #rl 