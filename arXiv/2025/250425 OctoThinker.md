https://natural-rugby-f7c.notion.site/OctoThinker-Revisiting-Mid-Training-In-the-Era-of-RL-Scaling-1d20b810e2d680c494a9f9dad0a90d53

*OctoThinker: Revisiting Mid-Training In the Era of RL Scaling* (Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu)

> 

미드트레이닝을 (Mid-Training) 사용해 Llama의 추론 RL 성능을 향상시킨 시도. 이전에도 Llama에서 추론 RL이 잘 되지 않는 이유를 분석하면서 비슷한 시도를 한 연구가 있었죠. (https://arxiv.org/abs/2503.01307) 고품질 데이터와 CoT QA, Instruction Following 데이터를 사용한 학습으로 성능이 높아진다는 결과입니다.

모델이 커졌을 때의 결과, R1에 DeepSeek V3의 데이터 큐레이션이 미친 영향 등이 궁금하긴 합니다.

<english>
Attempt of improving reasoning RL performance of Llama using mid-training. Previously there was a similar attempt, with analyzing why reasoning RL does not works well on Llama (https://arxiv.org/abs/2503.01307). The author suggests high quality data, CoT QA, and instruction following data improves the performance.

I wonder about the result when model scale became larger, and how data curation of DeepSeek V3 affects the results of R1.
</english>

#reasoning #rl 