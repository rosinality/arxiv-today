https://arxiv.org/abs/2504.13837

*Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?* (Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang)

> Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io

지금 가장 흥미로운 문제 중 하나인 RL이 베이스 모델의 성능을 확장하는 것인가, 혹은 베이스 모델에 잠재되어 있는 능력을 끌어내는 것인가에 대한 분석. 여기서는 베이스 모델에서도 나올 수 있는 샘플의 확률을 RL이 높여주는 것이고, 반대로 풀 수 있는 문제들의 범위는 줄어든다는 분석을 합니다. 오히려 Distillation이 문제의 범위를 확장시킬 수 있다고 주장하네요.

더 많은 규모의 연산을 투입하고 KL 페널티를 제거했을 때의 패턴까지도 고려해야 하긴 하겠지만, 여하간 프리트레이닝이 여전히 많은 것을 결정한다는 것은 사실 아닐까 싶네요.

<english>
Analysis on whether RL expands capability of pretrained models, or just exposing capabilities already existing it, which is one of most interesting problem nowadays. This paper suggests that RL increase probability of samples which is already possible to sample from base models, and in contrast of that, coverage of solvable problems is actually decreases. The authors suggest that distillation can expand set of solvable problems, in contrast.

We should consider the cases that when more computation is introduced and KL penalty is removed, but, anyway, it would be true that pretraining again determines many capabilities.
</english>

#rl #reasoning 