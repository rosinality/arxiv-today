https://arxiv.org/abs/2503.17924

*WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training* (Zheng Wang, Anna Cai, Xinfeng Xie, Zaifeng Pan, Yue Guan, Weiwei Chu, Jie Wang, Shikai Li, Jianyu Huang, Chris Cai, Yuchen Hao, Yufei Ding)

> In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for large language model training. We first thoroughly analyze the workload imbalance issue in LLM training and identify two primary sources of imbalance at the pipeline parallelism and context parallelism levels. Then, to address the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a workload-aware variable-length document packing method to balance the computation and communication workload across micro-batches. Additionally, at the context parallelism level, WLB-LLM introduces a novel fine-grained per-document sharding strategy, ensuring each worker within a context parallelism group has an identical workload. Comprehensive experiments under different model scales demonstrate that WLB-LLM significantly mitigates the workload imbalance during 4D parallelism LLM training and achieves an average speedup of 1.23x when applying WLB-LLM in our internal LLM training framework.

배치 내에 서로 다른 Context Length를 갖는 시퀀스들이 섞임으로 인해 발생하는 비효율성을 해소하려는 시도가 많이 나오네요. (https://arxiv.org/abs/2502.21231, https://arxiv.org/abs/2503.07680) 이쪽은 이전 시도들과 달리 짧은 시퀀스들을 잘 이어붙여서 해결해보자는 아이디어입니다.

<english>
There are many attempt to reduce inefficiencies arose from seqeuences with varying context lengths are packed in the batch (https://arxiv.org/abs/2502.21231, https://arxiv.org/abs/2503.07680). Contrary to previous attempts this research tried to concatenate short sequences to resolve it.
</english>

#long-context #efficiency #parallelism 