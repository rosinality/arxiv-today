https://arxiv.org/abs/2504.16677

*A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics* (Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder)

> In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.

과제, 언어, 모델 크기에 따른 다국어 성능 변화에 대한 분석. 이 요인들이 서로 상호작용합니다만 분명한 것은 모델이 커지면 특성이 상당히 달라진다는 것이네요.

<english>
Analysis on multilingual performance with respect to tasks, languages, and model sizes. Though these factors interacts with each other, but apparaently characteristic changes quite a lot when model sizes became larger.
</english>

#multilingual 