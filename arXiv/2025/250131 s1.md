https://arxiv.org/abs/2501.19393

*s1: Simple test-time scaling* (Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto)

> Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.

추론 과정이 포함된 1K 샘플로 학습시킨 다음 CoT의 길이를 강제로 변경하는 디코딩 방법으로 Inference-time Scaling. 생성을 바로 종료하려는 경향만 억제해도 흥미로운 패턴이 생긴다는 결과가 다시 생각나는군요. (https://arxiv.org/abs/2402.10200)

그나저나 o1 이후로 X1 스타일의 작명이 유행하는군요.

<english>
Inference-time scaling by decoding method of forcing the length of CoT, after training with 1K samples containing reasoning steps. It reminds me the result of interesting reasoning pattern can happen if we suppress the tendency of model to stop generation immediately. (https://arxiv.org/abs/2402.10200)

By the way, "X1" style naming is in vogue since the o1.
</english>

#reasoning #inference-time-scaling

# Links

[[240215 Chain-of-Thought Reasoning Without Prompting.md]]