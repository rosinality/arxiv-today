https://arxiv.org/abs/2502.13917

*TESS 2: A Large-Scale Generalist Diffusion Language Model* (Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan)

> We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2.

Diffusion LM을 요즘 생각보다 많이 하네요. 규모가 커지면 방법의 차이는 중요하지 않다고 하는 격언이 있지만 (https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/) 그래도 특성의 차이가 있을지 생각해보게 되죠.

요즘 Looped Transformer도 많이 나오더군요. (https://arxiv.org/abs/2502.13842, https://arxiv.org/abs/2502.13181) 대안적 아키텍처에 대한 관심은 늘 있는 것이긴 하지만요.

<english>
Many papers on diffusion LM is occuring. There is a quote saying difference in methods is not that important when it is scaled enough (https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/), but it makes me think whether there are qualitative differences.

By the way, recently there are many researches on looped transformers (https://arxiv.org/abs/2502.13842, https://arxiv.org/abs/2502.13181). Of course there is constant traction around alternative architectures.
</english>

#diffusion #llm 