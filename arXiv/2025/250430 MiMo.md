https://github.com/XiaomiMiMo/MiMo/blob/main/MiMo-7B-Technical-Report.pdf

*MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining* (Xiaomi LLM-Core Team)

> We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model’s reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional MultiToken Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty–driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.

샤오미의 추론 모델. 8B 모델을 25T 학습시켰군요. 프리트레이닝 단계에서부터 생성한 추론 데이터를 추가시켰습니다.

<english>
Reasoning model from Xiaomi. They pretrained 8B models on 25T tokens. They incorporated synthesized reasoning data at time time of pretraining.
</english>

#llm #reasoning #rl 