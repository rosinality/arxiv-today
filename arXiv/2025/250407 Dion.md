https://arxiv.org/abs/2504.05295

*Dion: A Communication-Efficient Optimizer for Large Models* (Kwangjun Ahn, Byron Xu)

> Training large AI models efficiently requires distributing computation across multiple accelerators, but this often incurs significant communication overhead -- especially during gradient synchronization. We introduce Dion, a communication-efficient optimizer that retains the synchronous semantics of standard distributed training (e.g., DDP, FSDP) while substantially reducing I/O costs. Unlike conventional optimizers that synchronize full gradient matrices, Dion leverages orthonormalized updates with device-local momentum buffers, eliminating the need for full gradient exchange. It further supports an efficient sharding strategy that avoids reconstructing large matrices during training.

Muon의 Low Rank 버전이네요.

<english>
Low rank Muon.
</english>

#optimizer 