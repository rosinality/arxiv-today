https://arxiv.org/abs/2509.09265

*Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents* (Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, Ke Wang)

> In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/

멀티 스텝 환경에서 엔트로피가 낮은 스텝은 강하게, 엔트로피가 높은 스텝은 약하게 학습되어야 하지만 오히려 반대의 경향이 존재. 따라서 Advantage를 엔트로피를 사용해 조정. 또한 Policy가 엔트로피가 낮은 스텝으로 연결되는 스텝을 선호하도록 학습.

In multi-step settings, steps with low entropy should be reinforced strongly, and those with high entropy should be reinforced weakly, but actually there is a reversed tendency. Thus, modulate the advantage estimate using entropy. Also, encourage the policy to prefer actions that lead to low entropy steps.

#rl #reasoning #agent 