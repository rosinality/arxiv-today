https://arxiv.org/abs/2501.08617

*RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation* (Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac)

> Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.

재미있는 문제의식이네요. 모델의 응답을 평가할 때 응답이 생성된 직후에 한다면 그 시점에 사람이 생각하는 응답의 유용성을 기반으로 평가하는 것이죠. 그러나 실제 응답의 가치는 그 응답에 따라 의사결정을 했을 때의 결과에 있죠.

그러나 의사결정을 했을 때의 결과를 알기는 어려우니 시뮬레이션 등을 통해 결과에 대해 좀 더 나은 판단을 할 수 있도록 한다면 어떨까 하는 아이디어입니다.

<english>
Interesting problem settings. If we evaluate model response immediately after the generation then it is based on the human's perception and prediction of utility. But actual value of the response will depend on the result after doing decision making based on it.

But as it is hard to know the result after decision making, idea of this paper is let the human to do more informed evaluation by tools like simulation.
</english>

#alignment 