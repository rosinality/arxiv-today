https://arxiv.org/abs/2507.13618

*Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters* (Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Zhichao Huang, Tao Li, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, Yonghui Wu)

> Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.

바이트댄스의 번역 모델. 병렬 코퍼스를 번역 모델을 사용해서 만들었다는 것을 고려하면 다국어 능력 강화를 위해 만든 것이 아닌가 싶네요. Cycle consistency를 사용해 번역에 대한 보상을 부여했는데 이쪽도 흥미롭군요.

<english>
ByteDance's translation model. Considering they built parallel corpus using translatmion model itself, it maybe it is for enhancing multilingual capability. They used cycle consistency for assigning rewards, which is also interesting.
</english>

#multilingual #synthetic-data #machine-translation 