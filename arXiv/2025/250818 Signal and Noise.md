https://arxiv.org/abs/2508.13144

*Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation* (David Heineman, Valentin Hofmann, Ian Magnusson, Yuling Gu, Noah A. Smith, Hannaneh Hajishirzi, Kyle Lo, Jesse Dodge)

> Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.

소규모의 실험 결과를 대규모의 학습에 적용하려면 판단 근거가 되는 벤치마크가 신호 - 서로 다른 모델의 점수 차이 - 대 잡음 - 학습 과정에 의한 점수의 분산 - 비가 높은 것이 이상적. 신호 대 잡음비를 높이려면 점수를 평균내거나 Perplexity로 척도를 바꾸는 것 등이 가능.

To apply small-scale experimental results to large-scale training, the benchmark used as decision criteria should ideally have a high signal - the difference in scores between different models - to noise - the variance in scores during the training process - ratio. To increase the signal-to-noise ratio we can average scores or change the metric to perplexity.

#benchmark #scaling-law 