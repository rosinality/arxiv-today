https://arxiv.org/abs/2501.13011

*MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking* (Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah)

> Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step "reward hacks") even if humans are not able to detect that the behaviour is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.

모델이 여러 단계에 걸쳐 관찰자가 포착하기 어려운 방식으로 행동해서 Reward Hacking을 하는 시나리오에 대한 연구. 모델이 바로 다음 단계에 대해서만 보상을 받되 관찰자의 모델의 행동에 대한 결과의 예측을 추가적인 보상으로 사용하는 형태입니다. 장기적인 결과 자체가 아니라 예측을 사용해서 장기적인 결과를 해킹하려는 시도를 차단한다는 흐름이네요.

<english>
The study on a scenario that models does reward hacking by doing subtle actions that hard to catch by the observer in multiple steps. They dealt with this problem by letting the model got a reward from immerdiate next consequences and prediction of the further consequences from the observer. The main idea is block hacking long therm rewards by using predictions instead of the result itself.
</english>

#reward #alignment #safety 