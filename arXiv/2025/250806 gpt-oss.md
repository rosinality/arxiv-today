https://openai.com/index/introducing-gpt-oss/

*gpt-oss* (OpenAI)

> 

- DeepSeek V3 등에 비교하자면 좀 더 좁고, 깊고, 대신 넓은 Expert를 가진 모델.

- 20B와 120B의 차이는 레이어의 수 (24 -> 36)와 Expert의 수 (32 -> 128). Kimi K2에서 그랬던 것처럼 주로 Expert의 크기를 증대시켜 모델을 확장했다고 할 수 있을 것이고, 그게 꽤 효과적이라는 의미일지도?

- Attention sink 사용. QK와 싱크 토큰을 cat([QK, sink], -1)으로 결합한 다음 Value와 곱하기 전에 softmax(cat([QK, sink], -1))[:-1]로 절단하는 방식. 모델 카드에서 언급한 것처럼 softmax-1과 비슷. 하지만 싱크는 임의의 값을 취할 수 있으니 학습이 더 쉬울지도.

- 극단적인 Window 128 Local Attention. 비율은 1:1. 대체로 이 수준에서는 성능 저하를 보고하고 있는데 그만한 가치가 있다고 생각한 것인지 혹은 Attention sink를 사용하는 것이 패턴을 바꾸는 것인지 의문. 여담이지만 이것에 대해 GPT-3를 인용하고 있음. GPT-3에서 자주 간과되는 디테일 중 하나.

- (clamp(w) + 1)(clamp(g) * sigmoid(1.702 * clamp(g))). sigmoid(1.702x)는 GELU에서 나온 패턴. Clamping은 양자화에서는 일반적인 방법. (w + 1)swish(g)는 비전형적인데 평균을 1로 조정하는 전형적인 트릭으로 생각할 수 있을지도. 마찬가지로 비전형적으로 Linear 레이어에 대부분 Bias를 사용하고 있음.

- H100 2.1M 시간을 gpt-oss-120b 프리트레이닝에 사용. MFU는 알 수 없지만 gpt-oss-120b가 60T 토큰 이상에 대해서 학습되었다는 의미일지도? (그럴 듯한 값.)

- 양자화 단계를 거쳐 Expert에 MXFP4를 사용. Expert에 대해서만 낮은 비트를 사용하는 것이 훨씬 다루기 쉽지 않을까 싶음.

- 포스트트레이닝에서 Role hierarchy를 부여. 포스트트레이닝에서 중요한 부분이라고 생각. 모델이 사용자의 입력을 스스로의 생성 결과보다 우선하는 것이 자연스러움.

- 이 디테일들을 제외하면 아키텍처는 꽤 평이. Pre-LN, RoPE + YaRN (NoPE 등은 쓰지 않음), 등등.
  
- Compared to other MoE models, like DeepSeek V3, gpt-oss is narrower, deeper, but has wider experts.

- The differences between 20B and 120B are the number of layers (24 -> 36) and experts (32 -> 128). As Kimi K2 did, they scaled the model mostly by increasing the number of experts, and this may suggest it is quite effective.

- Attention sink. QK and sink token are combined by cat([QK, sink], -1) and sliced before multiplying with the values by softmax(cat([QK, sink], -1))[:-1]. As they mentioned in the model card it is similar to softmax-1. But as the sink is allowed to take arbitrary values, it could be easier to train.

- Extreme local attention of window size 128 with a ratio of local to global of 1:1. Many studies have reported degradation at this level of locality. Maybe they found it's worth it? Or could incorporating attention sinks totally change the math? By the way, they cite GPT-3 for this sparse attention pattern, which is an often neglected detail for GPT-3.

- (clamp(w) + 1)(clamp(g) * sigmoid(1.702 * clamp(g))). sigmoid(1.702x) comes from GELU. Clamping is also conventional for quantization. (w + 1)swish(g) is quite unconventional, but it can be a common trick for mean centering around 1. Also bias is used almost everywhere for linear layers, which is also unconventional these days.

- They used 2.1M H100 hours for gpt-oss-120b. We don't know their MFUs, but maybe this means gpt-oss-120b was trained on over 60T tokens (Actually it feels quite reasonable).

- MXFP4 for experts, through a quantization step. I suspect it could be much easier to deal with lower bits on experts only.

- Role hierarchy for instructions. I think this is an important detail for post-training. The model should prioritize user inputs over its own generation.

- Besides the above, the architecture is quite plain. Pre-LN, RoPE with YaRN (No NoPE), and so on.

