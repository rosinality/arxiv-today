https://arxiv.org/abs/2512.04040

*RELIC: Interactive Video World Model with Long-Horizon Memory* (Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan)

> A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.

월드 모델 만들기. 언리얼 엔진으로 만든 생성 데이터를 사용해 액션으로 조건을 줄 수 있도록 Bidirectional Teacher를 튜닝. 그리고 Autoregressive Student에 Distill. 어쩌면 일반적인 비디오 생성 프리트레이닝과 액션 입력 튜닝 사이의 갭이 가장 중요한 문제일지도?

Building world models. Using synthetic data built with Unreal Engine tune the bidirectional teachers to allow action conditioning. Then distill this teacher into an autoregressive student. Maybe the gap between plain video generation pretraining and action conditioned tuning could be the most significant problem?

#video-generation #world-models #synthetic-data 