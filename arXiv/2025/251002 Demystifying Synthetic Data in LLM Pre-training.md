https://arxiv.org/abs/2510.01631

*Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls* (Feiyang Kang, Newsha Ardalani, Michael Kuchnik, Youssef Emad, Mostafa Elhoushi, Shubhabrata Sengupta, Shang-Wen Li, Ramya Raghavendra, Ruoxi Jia, Carole-Jean Wu)

> Training data plays a crucial role in Large Language Models (LLM) scaling, yet high quality data is of limited supply. Synthetic data techniques offer a potential path toward sidestepping these limitations. We conduct a large-scale empirical investigation (>1000 LLMs with >100k GPU hours) using a unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures of natural and synthetic data. Specifically, we found pre-training on rephrased synthetic data \textit{alone} is not faster than pre-training on natural web texts; while pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts can speed up 5-10x (to reach the same validation loss) at larger data budgets. Pre-training on textbook-style synthetic data \textit{alone} results in notably higher loss on many downstream domains especially at small data budgets. "Good" ratios of synthetic data in training data mixtures depend on the model size and data budget, empirically converging to ~30% for rephrased synthetic data. Larger generator models do not necessarily yield better pre-training data than ~8B-param models. These results contribute mixed evidence on "model collapse" during large-scale single-round (n=1) model training on synthetic data--training on rephrased synthetic data shows no degradation in performance in foreseeable scales whereas training on mixtures of textbook-style pure-generated synthetic data shows patterns predicted by "model collapse". Our work demystifies synthetic data in pre-training, validates its conditional benefits, and offers practical guidance.

교과서 스타일의 합성 데이터 생성은 도움이 되지 않음. Rephrasing 데이터는 30% 정도 섞었을 때 도움이 됨. 최적 비율은 모델이 커지면서 감소하는 경향이 있음. 아마 합성 데이터에 의한 성능 향상이 데이터의 노이즈 감소에서 발생한다는 의미일 수 있음. 다만 실험은 필터링 없는 CC 코퍼스에 대해서 수행됨.

Textbook style synthetic data does not work. rephrasing works when mixed at about 30% ratio. Its optimal ratio reduces as the model is larger. I think this suggests the effect of synthetic data can be driven by less "noise". But experiments are done with unfiltered CC.

#synthetic-data #pretraining 