https://arxiv.org/abs/2504.03612

*AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset* (Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, Ganqu Cui, Wanxiang Che, Zhiyuan Liu, Maosong Sun)

> Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \textbf{A}nnotations, \textbf{I}nstructions, and \textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment.

Preference 데이터셋 구축에서 어노테이션 방법, Instruction 선택, Response Pair 생성 각각을 분석. 일단 Judge Model이 있다는 가정 하에서의 분석이긴 합니다. LLM에 따른 응답의 점수 편차가 작은 Instruction을 선택하고, 점수가 높으면서 Pair 사의 차이가 적당한 Pair를 선택하라고 하는군요.

<english>
Analyzing each steps during preference dataset constructions, namely annotation methods, choosing instructions, generating response pairs. It is under the assumption of judge model exists. The authors insist that it is better to choose instruction with lower variance of scores between LLMs, and pair with high scores and difference between pairs are moderate.
</english>

#reward-model #alignment 