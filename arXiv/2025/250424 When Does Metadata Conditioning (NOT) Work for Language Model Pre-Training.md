https://arxiv.org/abs/2504.17562

*When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars* (Rei Higuchi, Ryotaro Kawata, Naoki Nishikawa, Kazusato Oko, Shoichiro Yamaguchi, Sosuke Kobayashi, Seiya Tokui, Kohei Hayashi, Daisuke Okanohara, Taiji Suzuki)

> The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.

프리트레이닝에 문서 메타데이터를 사용하는 것에 대한 분석. (https://arxiv.org/abs/2501.01956) 인공적인 세팅이긴 합니다. 메타데이터를 사용하면 메타데이터와 관련된 정보를 추출하는 능력은 약해지겠죠. 프리트레이닝의 문제는 어렵다는 것이 아니라 쉽다는 것에 있지 않나 싶네요.

<english>
Analysis on using document metadata for pretraining (https://arxiv.org/abs/2501.01956). It is rather artificial setting. Of course using metadata will reduce the capability of extracting metadata-related informations. I think the problem of pretraining is laid in it is easy, not hard.
</english>

#pretraining 