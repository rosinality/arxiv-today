https://arxiv.org/abs/2502.11886

*LIMR: Less is More for RL Scaling* (Xuefeng Li, Haoyang Zou, Pengfei Liu)

> In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR.

SFT에 이어 RL에도 많은 샘플이 필요하지 않다는 결과. (https://arxiv.org/abs/2502.03387) 모델의 전반적인 학습 경향과 경향이 비슷한 샘플을 찾는 방식이군요. Kimi k1.5의 Curriculum도 생각나네요. (https://arxiv.org/abs/2501.12599)

또한 RL이 SFT보다 더 샘플 효율적이었다고 합니다.

<english>
Along with SFT, the results that RL also does not requires a large amount of samples  (https://arxiv.org/abs/2502.03387). The main method is find out samples of similar trends with overall training dynamics of the model. It reminds me curriculum used in Kimi k1.5 (https://arxiv.org/abs/2501.12599).

The authors also says RL is more sample efficient than SFT.
</english>

#rl #reasoning 