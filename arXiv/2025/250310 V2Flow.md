https://arxiv.org/abs/2503.07493

*V2Flow: Unifying Visual Tokenization and Large Language Model Vocabularies for Autoregressive Image Generation* (Guiwei Zhang, Tianyu Zhang, Mohan Zhou, Yalong Bai, Biye Li)

> We propose V2Flow, a novel tokenizer that produces discrete visual tokens capable of high-fidelity reconstruction, while ensuring structural and latent distribution alignment with the vocabulary space of large language models (LLMs). Leveraging this tight visual-vocabulary coupling, V2Flow enables autoregressive visual generation on top of existing LLMs. Our approach formulates visual tokenization as a flow-matching problem, aiming to learn a mapping from a standard normal prior to the continuous image distribution, conditioned on token sequences embedded within the LLMs vocabulary space. The effectiveness of V2Flow stems from two core designs. First, we propose a Visual Vocabulary resampler, which compresses visual data into compact token sequences, with each represented as a soft categorical distribution over LLM's vocabulary. This allows seamless integration of visual tokens into existing LLMs for autoregressive visual generation. Second, we present a masked autoregressive Rectified-Flow decoder, employing a masked transformer encoder-decoder to refine visual tokens into contextually enriched embeddings. These embeddings then condition a dedicated velocity field for precise reconstruction. Additionally, an autoregressive rectified-flow sampling strategy is incorporated, ensuring flexible sequence lengths while preserving competitive reconstruction quality. Extensive experiments show that V2Flow outperforms mainstream VQ-based tokenizers and facilitates autoregressive visual generation on top of existing. https://github.com/zhangguiwei610/V2Flow

Masked Autoregression에 (https://arxiv.org/abs/2406.11838) Rectified Flow를 사용해 이미지 토크나이저를 학습. 이때 이미지 토크나이저의 코드북을 LLM의 임베딩을 사용해서 만듭니다. LLM을 튜닝할 때 텍스트 토큰과 이미지 토큰을 정렬시키려는 아이디어겠죠.

이미지 토크나이저를 Diffusion을 사용해 구성하는 것은 흥미로운 아이디어라고 봅니다. (https://arxiv.org/abs/2501.18593) 

<english>
Training image tokenizers using masked autoregression (https://arxiv.org/abs/2406.11838) and rectified flow. For this the authors built codebook of image tokenizers using embedding of LLMs. It is to align text and image tokens when they are going to tune LLMs.

I think constructing image tokenizers using diffusion is intruging idea (https://arxiv.org/abs/2501.18593).
</english>

#tokenizer #vq #autoregressive-model #diffusion 