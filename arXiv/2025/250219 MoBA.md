https://github.com/MoonshotAI/MoBA/blob/master/MoBA_Tech_Report.pdf

*MoBA: Mixture of Block Attention for Long-Context LLMs* (Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, Jiezhong Qiu)

> Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in com- plex reasoning tasks remains inadequately explored.
> In this work, we propose a solution that adheres to the “less structure” principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We intro- duce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates su- perior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi’s long-context requests and demon- strates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/moba.

R1에 이어 이번에도 Moonshot AI가 DeepSeek과 비슷한 시점에 비슷한 논문을 공개했네요. Sparse Attention입니다. NSA와 비슷하게 (https://arxiv.org/abs/2502.11089) 시퀀스를 블럭으로 쪼개고 풀링으로 대표 토큰을 만든 다음 Top-K 블럭을 뽑는 방식입니다.

<english>
Moonshot AI relasesd similar paper to DeepSeek, like when R1 was released. It is a sparse attention. Similarto NSA (https://arxiv.org/abs/2502.11089) it chunks sequences into blocks and get representative tokens using pooling, and the select top-K blocks.
</english>

#sparsity #efficiency #efficient-training 