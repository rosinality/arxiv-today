https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf

*ERNIE 4.5 Technical Report* (ERNIE Team, Baidu)

> In this report, we introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality. This MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the PaddlePaddle deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resourceefficient training and inference workflows, and multi-hardware compatibility.

바이두의 멀티모달 파운데이션 모델. 학습과 추론 인프라에 대해서까지 굉장히 상세하네요. MoE 라우터 직교화가 벌써 쓰이고 있군요. LR 스케줄을 EMA로 완전히 대체하는 것도 채택했네요.

<english>
Baidu's multimodal foundation model. It is very detailed on training and inference infrastructure. MoE router orthogonalization is already used. They also adopted completely replacing LR schedules with EMA.
</english>

#llm #multimodal #pretraining #moe #post-training 