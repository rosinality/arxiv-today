https://dapo-sia.github.io/static/pdf/dapo_paper.pdf

*DAPO: an Open-Source LLM Reinforcement Learning System at Scale* (ByteDance Seed, Institute for AI Industry Research (AIR), Tsinghua University, The University of Hong Kong, SIA-Lab of Tsinghua AIR and ByteDance Seed)

> Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework a, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.

바이트댄스의 LLM RL 알고리즘. GRPO에 KL Penalty 제외, Clipping 범위에서 상한과 하한을 분리, 정확도가 0 혹은 1인 샘플 제외, 샘플 길이에 따른 가중치 적용을 추가하고 그리고 Truncation이 일어난 샘플에 대한 페널티를 설계했네요.

앞으로도 온갖 알고리즘이 등장하겠죠. 다만 DPO 시리즈보다는 재미있네요.

<english>
LLM RL algorithm from ByteDance. Based on GRPO, exclusion of KL penalty, separating upper and lower bound of clipping ranges, exclusion of samples with accuracy is 0 or 1, and weighting depend on samples length are added. And they designed penalty for samples with truncation.

Various algorithms would appear in future. But it is more interesting to me than DPO variants.
</english>

#reasoning #rl 