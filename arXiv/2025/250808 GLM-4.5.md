https://arxiv.org/abs/2508.06471

*GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models* (GLM-4.5 Team)

> We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.

기존의 중국 MoE 모델보다 좁고 깊은 모델. 미드트레이닝을 합쳐 23T 학습. 소위 Agentic RL은 이제 파운데이션 모델의 기본 소양.

A deeper and narrower MoE model compared to previous chinese models. Trained on a total of 23T tokens including mid-training. Agentic RL is now a basic requirement for foundation models.

#moe #rl #reasoning 