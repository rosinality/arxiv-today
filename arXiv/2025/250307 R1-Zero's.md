https://arxiv.org/abs/2503.05132

*R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model* (Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh)

> Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero

Qwen 2 VL 2B 기반으로 멀티모달 추론 RL을 시도. SFT 없이 GRPO로 추론 능력의 획득을 관찰할 수 있었다고 하네요. 역으로 Instruct 모델로는 잘 되지 않았다고 하는군요.

<english>
The authors tried multimodal reasoning RL based on Qwen 2 VL 2B. They observed acquisition of reasoning abilities using GRPO without SFT. Conversely, they say that it does not very well on instruct models.
</english>

#reasoning #rl #multimodal 