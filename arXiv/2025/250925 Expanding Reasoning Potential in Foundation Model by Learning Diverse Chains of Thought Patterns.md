https://arxiv.org/abs/2509.21124

*Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns* (Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, Xunliang Cai)

> Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.

코어셋과 비슷한 샘플을 찾는 것으로 미드트레이닝에 사용할 데이터를 선택. 유사도는 추론 패턴과 엔트로피에 대한 DTW로 측정 (!). 이전의 Instruction 데이터 선택과 비슷한 문제일 수 있지만 문제를 이후 RL에 도움이 되는 데이터를 선정하는 것으로 간주한다면 좀 더 흥미로운 문제가 될지도.

Selecting data for mid-training by finding similar samples to a chosen core set, measured by DTW (!) on reasoning patterns and entropy. This could be similar to previous instruction data selection problems, but if formulated as selecting samples that are beneficial for later RL training, it could be more interesting.

#rl #reasoning #mid-training 