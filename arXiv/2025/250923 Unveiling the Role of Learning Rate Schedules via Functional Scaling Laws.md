https://arxiv.org/abs/2509.19189

*Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws* (Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu)

> Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.

리스크를 감소 불가능한 리스크, 모델 규모, 누적 LR, 그리고 LR과 배치 크기로 보정된 그래디언트 노이즈의 컨볼루션 항으로 분해. 이에 기반해 LR 스케줄의 특성을 분석하고 LR Scaling Law를 구성 (https://arxiv.org/abs/2408.11029, https://arxiv.org/abs/2503.12811).

Decompose risk into irreducible risk, model scaling, accumulated LR, and a convolution term of gradient noise adjusted by LR and batch size. Based on this they analyzed the characteristics of LR schedules and constructed LR scaling laws (https://arxiv.org/abs/2408.11029, https://arxiv.org/abs/2503.12811).

#scaling-law #optimization 