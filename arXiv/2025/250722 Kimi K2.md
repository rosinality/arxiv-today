https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf

*Kimi K2: Open Agentic Intelligence* (Kimi Team)

> We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
> Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual — surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints1 to facilitate future research and applications of agentic intelligence.

프리트레이닝 데이터 효율성을 위해 Rephrasing을 사용. Multi Epoch 학습을 한다면 Rephrasing을 하는 쪽이 효과적이라는 결론. Rephrasing은 이전부터 계속 시도된 방법이지만 대규모로 사용된 사례는 최초.

MoE Sparsity Scaling Law. 현 구조에서는 Kimi K2의 Sparsity 48이 한계인 듯.

Interleaved 1F1B 기반 All-to-All Overlap. Activation Offloading. 이쪽도 흥미로운 선택.

Rubric 기반 Reward Model. 이쪽이 원래는 포스트트레이닝의 요점이었는데 Verifiable Reward에 상당히 밀려난 모습. Verifiable Reward로 Reward Model 또한 학습시킴.

여전히 Synchronized RL을 사용 중. Sync vs Async RL의 대규모 학습에서의 결과 차이도 흥미로운 부분.

Used rephrasing for pretraining data efficiency. The conclusion is that rephrasing is effective especially for multi-epoch. Rephrasing has been tried many times before, but this is the first case of it being applied at large scale.

MoE sparsity scaling law. For the current architecture, Kimi K2's sparsity of 48 seems to be the upper limit.

Interleaved 1F1B-based all-to-all overlap. Also activation offloading. Interesting choices.

Rubric-based reward model. This was the main focus of post-training, but now overshadowed by verifiable rewards. The reward model is also trained using verifiable rewards.

Still using synchronized RL. The difference in results between sync vs async RL in large-scale training is also an interesting problem.

#llm #moe #scaling-law #synthetic-data #agent #pretraining 