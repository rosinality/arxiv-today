https://arxiv.org/abs/2511.11881

*Better LLM Reasoning via Dual-Play* (Zhengxin Zhang, Chengyu Huang, Aochong Oliver Li, Claire Cardie)

> Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

RL 과정에서 문제 생성 모델을 결합. 프리트레이닝 코퍼스로 문제 생성 모델에 시드를 주고 어렵지만 풀 수 있는 문제를 풀 수 없는 문제보다 더 크게 보상하는 난이도 보상을 사용. 그저 어려운 것이 아니라 흥미로운 문제를 생성할 수 있는 것이 중요할 것.

Incorporating problem generator (proposer) for RL. They used a pretraining corpus to give a seed for the proposer, and incorporated a difficulty reward that rewards hard, but solvable problems more than completely unsolvable problems. It is definitely important to generate not just hard but also interesting problems.

#rl #reasoning 