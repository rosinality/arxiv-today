https://arxiv.org/abs/2501.09755

*Learnings from Scaling Visual Tokenizers for Reconstruction and Generation* (Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen)

> Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.

이미지와 비디오 토크나이저의 인코더 및 디코더의 크기, 인코딩된 코드의 크기, Loss에 대한 분석. 전반적으로는 Reconstruction을 잘 하는 것이 더 나은 생성 모델로 이어지지는 않는다는 결론입니다. 중요한 문제는 코드의 크기가 커지면 생성 모델이 모델링하기 어려워하는 문제, 인코더가 커지면서 코드가 복잡해지는 효과, 디코더가 커지면서 코드에서는 정보가 빠지고 그 부분을 디코더가 커버해버리는 효과 등이 겹치는 것 같네요.

<english>
Analysis on effect of encoder or decoder sizes, sizes of encoded codes, and loss of image and video tokenizer. Overall the conclusion is better reconstruction does not result in better generation. It seems important problems are it is hard for generative model to predict if encoded code is larger, scaling encoder causes complex codes, and decoder scaling allows code to skip some informations and instead fill it using generation capability of the decoder.
</english>

#tokenizer #vae #image-generation #video-generation 