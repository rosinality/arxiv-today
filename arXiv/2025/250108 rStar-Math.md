https://arxiv.org/abs/2501.04519

*rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking* (Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang)

> We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.

코드 생성과 검증을 기반으로 MCTS. 각 스텝에 대한 평가는 정답으로 이어지는 스텝과 오답으로 이어지는 스텝에 대한 Preference로 학습된 Process Reward Model을 사용. MCTS 롤아웃으로 정책 모델을 학습하고, 이 정책 모델로 생성한 롤아웃으로 PRM을 학습하고, PRM을 기반으로 MCTS를 수행하는 방법으로 자기 개선.

<english>
MCTS based on code generation and verification. Evaluation on each step is performed with process reward model which is trained on preference between steps that conclude into correct or incorrect answers. Use self improvement recipe that first train a policy model with MCTS rollouts, then train a PRM with rollouts generated by this policy model, then doing a MCTS using this PRM.
</english>

#reasoning #mcts #search 