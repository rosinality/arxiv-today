https://arxiv.org/abs/2508.13148

*MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models* (Haoyu He, Katrin Renz, Yong Cao, Andreas Geiger)

> Diffusion language models, as a promising alternative to traditional autoregressive (AR) models, enable faster generation and richer conditioning on bidirectional context. However, they suffer from a key discrepancy between training and inference: during inference, MDLMs progressively reveal the structure of the generated sequence by producing fewer and fewer masked tokens, whereas this structure is ignored in training as tokens are masked at random. Although this discrepancy between training and inference can lead to suboptimal performance, it has been largely overlooked by previous works, leaving closing this gap between the two stages an open problem. To address this, we frame the problem of learning effective denoising trajectories as a sequential decision-making problem and use the resulting framework to apply reinforcement learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to exploit the Markov property diffusion possesses and explicitly train the model under the same progressive refining schedule used at inference. MDPO matches the performance of the previous state-of-the-art (SOTA) method with 60x fewer gradient updates, while achieving average improvements of 9.6% on MATH500 and 54.2% on Countdown over SOTA when trained within the same number of weight updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in inference replacement to overcome the limitation that the model cannot refine tokens flexibly. This simple yet effective training-free strategy, what we refer to as RCR, consistently improves performance and yields additional gains when combined with MDPO. Our findings establish great potential for investigating the discrepancy between pre-training and inference of MDLMs. Code: https://github.com/autonomousvision/mdpo. Project Page: https://cli212.github.io/MDPO/.

Masked Diffusion LM에서 중간에 나왔던 정답이 이후에 오답으로 덮어씌워지는 문제에 대한 분석 (https://arxiv.org/abs/2508.09138). 저자는 랜덤 마스킹을 사용하는 학습과 Confidence를 사용하는 샘플링 사이의 갭 문제라고 추측.

The problem in masked diffusion LMs where intermediate correct answers are overwritten by subsequent incorrect answers (https://arxiv.org/abs/2508.09138). The authors suspect that it is due to the gap between training, which uses random masking, and inference, which uses confidence.

#diffusion #rl 