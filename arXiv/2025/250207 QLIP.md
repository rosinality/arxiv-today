https://arxiv.org/abs/2502.05178

*QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation* (Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krähenbühl, De-An Huang)

> We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.

BSQ 기반 이미지 토크나이저를 (https://arxiv.org/abs/2406.07548) CLIP과 Reconstruction Loss를 기반으로 학습. 이 토크나이저를 통해 Image-Text Autoregressive 모델을 학습해봤군요.

<english>
Trained BSQ (https://arxiv.org/abs/2406.07548) based image tokenizers with CLIP and reconstruction loss. The authors trained autoregressive image-text models with this tokenizer.
</english>

#autoregressive-model #clip #vq

# Links

[[240611 Image and Video Tokenization with Binary Spherical Quantization.md]]