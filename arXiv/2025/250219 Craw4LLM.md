https://arxiv.org/abs/2502.13347

*Craw4LLM: Efficient Web Crawling for LLM Pretraining* (Shi Yu, Zhiyuan Liu, Chenyan Xiong)

> Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Crawl4LLM.

프리트레이닝 데이터 구축을 위한 웹 크롤러에서 크롤링 우선 순위를 문서 퀄리티 필터로 한다는 아이디어군요. 자체 크롤러까지 구축하는 쪽에서는 이런 고민도 있을지 모르겠네요.

<english>
The idea of set priority of crawling using document quality classifiers when building web crawler for constructing pretraing corpus. Maybe organizations that build internal crawlers have these kind of problems.
</english>

#dataset #corpus 