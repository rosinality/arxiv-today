https://arxiv.org/abs/2503.06749

*Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models* (Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, Shaohui Lin)

> DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .

멀티모달 RL. R1에서 샘플링한 데이터로 콜드 스타트를 했네요. 그냥 RL을 진행하면 무작정 응답이 길어지는 경향이 있어서 샘플링 길이를 점진적으로 증가시키는 방법을 썼다고 합니다. 이전에 나온 결과와 비교하자면 (https://arxiv.org/abs/2503.05132) 이쪽은 Instruct 모델을 베이스로 사용했네요.

<english>
Multimodal RL. They have cold started with data sampled from R1. If we just applies RL on it there is tendency of response length increases in uncontrolled manner, they have adopted method of increases sampling lengths gradually. Compared to previous results (https://arxiv.org/abs/2503.05132) this study have used instruct model as a base.
</english>

#multimodal #reasoning #rl 