https://arxiv.org/abs/2508.08221

*Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning* (Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Siran Yang, Jiamang Wang, Wenbo Su, Bo Zheng)

> Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.

Reward Normalization, Clip Higher 같은 그동안 누적된 추론 RL 트릭들에 대한 청소 작업. Sequence vs Token level Aggregation이 베이스 모델과 Instruction 모델에 대해 다른 패턴을 보이는 것과 같은 까다로운 문제들이 발견됨.

Cleaning up accumulated reasoning RL tricks such as reward normalization and clip higher. They discovered delicated issues, like how sequence vs. token-level aggregation exhibits different patterns between base and instruction models.

#rl #reasoning 