https://arxiv.org/abs/2503.04725

*L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling* (Zhuo Chen, Oriol Mayné i Comas, Zhuotao Jin, Di Luo, Marin Soljačić)

> We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L$^2$M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.

텍스트 시퀀스를 둘로 나눴을 때 두 서브시퀀스 사이의 Mutual Information은 시퀀스의 길이에 대해 Power Law를 따른다는 결과에서 Long Context 모델의 상태 크기는 최소한 시퀀스 길이에 대해 Power Law Scaling을 해야 한다는 결론을 유도.

따라서 고정 크기 상태를 사용하는 방법으로는 한계가 있을 수밖에 없겠죠. 여러 레이어를 사용하는 경우에도 상태의 크기는 레이어의 수에 대해 선형적으로 증가하니 한계가 있겠군요. Dynamic한 Sparse Attention에 대해서는 어떨까요?

<english>
The authors suggest the state size of long context model should be scaled with power law respect to sequence lengths, as mutual information between two subsequences in text sequence follows power law.

So the method of using fixed sized states has inherent limitations. Even when there are multiple layers, state sizes would scaled linearly. How this would applied to dynamic sparse attentions?
</english>

#long-context 