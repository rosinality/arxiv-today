https://arxiv.org/abs/2502.09509

*EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling* (Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis)

> Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.

Augmentation으로 VAE의 Latent의 구조를 개선하자는 아이디어. Latent의 구조를 개선하자는 것은 최근 많이 나오는 아이디어죠. (https://arxiv.org/abs/2501.09755, https://arxiv.org/abs/2502.03444) Semantic한 정보를 결합하는 형태의 접근들이 사실 Semantic한 정보 이전에 Equivariance 같은 보다 기본적인 측면에서도 개선해주는 효과가 있었던 것이 아닌가 싶네요.

<english>
The idea of improve structure of latents from VAE using augmentation. Improving structure of latents is recently popular approaches  (https://arxiv.org/abs/2501.09755, https://arxiv.org/abs/2502.03444). Maybe approach in form of incorporating semantic informations have the effect of improve basic properties like equivariance, before adding semantic informations.
</english>

#vae #tokenizer 