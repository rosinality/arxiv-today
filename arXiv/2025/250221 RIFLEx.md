https://arxiv.org/abs/2502.15894

*RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers* (Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, Jun Zhu)

> Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}

비디오 생성 모델의 길이 증가. 반복이 일어나는 프레임을 찾아 주기가 그와 맞는 RoPE 컴포넌트를 찾고 주파수를 조정하는 방식이군요.

<english>
Length extrapolation for video generation models. First find frames repetition occur then find out RoPE dimensions has prediods matching to it, and then adjusts its frequency.
</english>

#video-generation #long-context #positional-encoding 