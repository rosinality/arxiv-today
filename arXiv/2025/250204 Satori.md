https://arxiv.org/abs/2502.02508

*Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search* (Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan)

> Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.

검증이나 탐색 같은 메타 액션을 설정하고 오답인 경우에 강제로 백트랙시킨 다음 검증하게 하는 형태로 추론 능력을 학습시켰군요.

R1이 가장 단순한 형태로 이런 메타 액션이 등장할 수 있다는 것을 증명한 이후 추론 연구는 그를 기준으로 할 수밖에 없게 됐네요. 거기에 모델이 작아서 효과가 잘 나타나지 않은 것이 아닌가 하는 이야기도 계속 나오겠죠.

<english>
Starting with setting meta actions like reflection and explorations, and train reasoning ability by forcifully backtrack and let model to validate when answer is incorrect.

After R1 proved these kind of meta actions could appear from most basic form of training reasoning researches are now became to start from that point. Also, there would be continuous criticisms saying effect is not apparent due to small model sizes.
</english>

#reasoning #rl 