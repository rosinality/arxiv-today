https://github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf

*Kimi k1.5: Scaling Reinforcement Learning with LLMs* (Kimi Team)

> Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities—e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista—matching OpenAI’s o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results—e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench—outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%). The service of Kimi k1.5 on kimi.ai will be available soon.

Moonshot의 RL CoT 모델. DeepSeek과 마찬가지로 PRM이나 트리 서치 등에 의존하지 않는 정답 기반의 RL 방법입니다. 이쪽이 사용한 데이터 등에 대해서 좀 더 많이 밝히고 있네요. 그리고 멀티모달 문제에 대해서도 학습했군요.

추론 문제에 대한 접근은 그동안 트리 서치나 PRM에 많이 현혹됐죠. 트리 서치도 PRM도 CoT에 일정한 구조를 부과하는 것이고 그것에서 발생하는 한계가 있을 겁니다. 물론 이 두 방법이 전혀 도움이 되지 않는다고 확언할 수는 없겠죠. 그렇지만 더 단순한 방법을 먼저 시도하는 것이 순서일 것입니다.

<english>
RL CoT model from Moonshot. Like DeepSeek they employed correctness based RL without depending on PRM or tree search. This paper mentions more on data they used. And they did trained the model on multimodal tasks.

Tree search and PRM attracted a lot of interest on approaches for reasoning problems. As tree search or PRM assigns some structure on CoT it would have limitation from that. But it does not mean that we can say these approaches does not help at all. But it is right order to try more simpler methods.
</english>

#reasoning #rl #multimodal 