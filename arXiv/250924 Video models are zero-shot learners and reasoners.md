https://arxiv.org/abs/2509.20328

*Video models are zero-shot learners and reasoners* (Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos)

> The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.

Veo 3의 제로샷 능력. 이런 능력을 발현시키기 위한 데이터는 어떠한 것일지? 그리고 이런 비디오 생성 모형이 시각적 추론 문제를 어느 정도까지 풀 수 있을지? 흥미로운 문제일 듯.

Zero-shot capabilities of Veo 3. What kind of data is needed to unlock these capabilities? And to what degree can these video generation models solve visual reasoning problems? It could be an interesting problem.

#video-generation 