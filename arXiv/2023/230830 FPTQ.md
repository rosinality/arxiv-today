https://arxiv.org/abs/2308.15987

FPTQ: Fine-grained Post-Training Quantization for Large Language Models (Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, Yuchen Xie)

W4A8 quantization 결과가 하나 나왔군요. 자주 나오는 activation의 scale을 weight로 옮겨주는 트릭과 groupwise quantization 조합이군요.

4 bit quantization에 발생하는 perplexity 손실이 감수 가능한 정도인가 하는 질문이 다시 떠오르긴 하네요. (애초에 여기선 perplexity 자체를 보고하지 않고 있습니다만.) 큰 모델을 quantization 하는 것이 더 작은 모델을 쓰는 것보다 낫다는 표현을 하긴 하지만, 반대로 생각하면 가장 고성능의 모델을 제공하고 싶은 상황에서는 별 소용이 없다는 의미이기도 하죠.

#quantization 