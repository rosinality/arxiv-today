https://arxiv.org/abs/2310.07096

Sparse Universal Transformer (Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan)

Universal Transformer가 다시 눈에 띄네요. Universal Transformer처럼 레이어를 재사용하되 레이어 내의 Attention과 FFN을 모두 MoE로 바꿨군요. 추가로 새로운 Halting 방법을 제안했습니다. Universal Transformer 계통의 방법이 파라미터 효율성을 넘어 Transformer LM의 성능 향상에 도움이 될 수 있을지가 궁금해지네요.

#transformer 
