https://arxiv.org/abs/2311.16867

The Falcon Series of Open Language Models (Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo)

Falcon 모델에 대한 리포트. 모델 구축을 위한 의사 결정 과정이나 학습 방법 등에 대해서 굉장히 상세하네요.

한계라는 타이틀로 어쩌면 다음 학습에서는 개선하고 싶은 부분들일 것 같은 아이템들이 나와 있는데 이쪽도 재미있습니다. 웹 데이터 사용이나 제한적인 다국어, 코드 데이터 사용 등도 Ablation으로 얻은 결과인데 이 Ablation에 쓴 규모가 작아서 혹시 놓친 부분이 있지는 않았을까? 좀 더 많은 학습 토큰 수를 가져가고 코드 데이터의 비율을 높이는 게 좋지 않았을까? 같은 이야기를 하고 있네요.

#llm 