https://arxiv.org/abs/2312.09244

Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking (Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, Jonathan Berant)

프리트레이닝 혹은 파인튜닝에서 시드가 다른 Reward 모델들은 In distribution에서는 성능이 비슷하지만 OOD에서는 성능이 다르다는 발견. 앙상블, 특히 프리트레이닝 시점부터 시드가 다른 모델들의 앙상블이 이 문제를 완화시켜 주지만 Reward Hacking을 완전히 막지는 못하더라는 결과입니다. 뭔가 Reward 모델의 규모가 더 커지면 나아질까 싶긴 하네요.

#alignment 