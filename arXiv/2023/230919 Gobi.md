https://www.theinformation.com/articles/openai-hustles-to-beat-google-to-launch-multimodal-llm

OpenAI는 들리는 루머에 의하면 from scratch multimodal 모델을 계속 준비하고 있는 모양. Flamingo 이후 인기 있는 형태인 frozen image encoder - frozen text decoder 조합과는 다른 형태인데 이유가 뭘까 생각해보면...frozen text decoder를 쓰는 모델은 frozen image encoder가 생성한 임베딩을 이미 형성된 텍스트 공간에 끼워 넣는 방식으로 동작하게 된다고 알려져 있다. 그런데 from scratch 학습을 한다면 이 텍스트 공간 자체가 이미지에서 발생하는 정보와 결합되어 형성될 것이고, 이게 단순히 성능적인 이점 뿐만 아니라 모델의 특성 자체를 꽤 바꿀 수 있지 않을까 싶다.

물론 이미지와 텍스트는 대체로 추상성에서 굉장한 차이가 있어서 (아이가 놀고 있는 사진과 아이가 놀고 있다라는 서술의 구체성의 차이를 한 번 생각해보자. 아이가 노는 방식은 아주 구체적으로 이미지에 나타나 있겠지만, 논다라는 단어는 지극히 추상적이다.) 이걸 그대로 결합하는 것이 기대처럼 잘 되지는 않는다고 하는데...그 부분에서 큰 진전이 있었을 수도 있지 않겠나.

