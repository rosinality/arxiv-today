Phi-1.5에 대한 dataset contamination 가능성에 대한 Susan Zhang의 지적에 (https://x.com/suchenzang/status/1701615026648605095) 대해 Phi-1.5의 저자가 반박했군요. (https://x.com/EldanRonen/status/1701815969277259870) 아마 이 논란은 좀 더 갈 것 같습니다. (https://x.com/suchenzang/status/1702004679222378751)

데이터셋, 데이터셋을 어떻게 구축했는가에 대한 정보 자체도 공개하고 있지 않은 상황이라 응답이 다소 방어적이고 그런 현상이 나타나지 않았다 정도인 것 같네요. 물론 데이터셋이 공개되어 있다고 해도 이 문제에 대해 결론을 내기는 까다롭겠죠. 어쩌면 새로운 벤치마크나 실제 파인튜닝을 통한 활용 시에 어떤 결과를 보여주는지를 통해 간접적으로 평가하는 것이 나을 수도 있겠다 싶습니다.

사람들이 열광하고 있는 아이디어인 더 높은 퀄리티의 데이터셋을 사용하면 더 적은 양의 데이터로 더 작은 모델을 사용해도 고성능을 낼 수 있다는 발상 자체는 어떨까요? 그럴 것 같기는 합니다. 큰 모델의 필요성도 노이즈가 많은 데이터의 문제로 인해 더 강화되는 측면이 있을 수 있겠죠. 물론 현재 높은 퀄리티의 데이터라는 것이 그 노이즈가 많은 데이터를 큰 모델로 대규모로 학습한 다음 instruction tuning과 rlhf를 거친 모델의 결과라는 점에서 "높은 퀄리티의 데이터"라고만 말하는 것이 적절한가 싶긴 합니다만.

#dataset 