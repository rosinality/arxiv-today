https://arxiv.org/abs/2312.07533

VILA: On Pre-training for Visual Language Models (Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han)

Vision-Language 모델의 프리트레이닝과 파인튜닝에 대한 발견들. 프로젝터를 사용하는 시나리오에서는:

1. LLM 얼리기 vs LLM 파인튜닝하기. 파인튜닝하는 것이 낫다. 특히 in-context learning에서 성능 차이가 많이 난다. 프로젝터는 단순한 linear model을 쓰는 쪽이 낫다. 파인튜닝을 했을 때 이미지와 텍스트 임베딩이 더 잘 정렬되기 때문인 것으로 추측. 이미지/텍스트 expert를 분리하는 것보다 파인튜닝이 효과적.
2. interleaved 데이터셋 사용은 필수적. 텍스트 성능이 폭락. 그래도 성능이 떨어지는 것을 막을 수는 없으니 텍스트 데이터(instruction) 추가가 바람직.
3. 이미지 크기는 중요하지만 토큰이 아깝다면 downsampling을 고려해볼만 하지 않을까.

전반적으로 LLM을 얼려놓는 것으로는 한계가 있다는 것을 다시 한 번 확인한 것 같네요. 텍스트와 멀티모달 데이터를 같이 학습하는 형태의 학습 계획을 만드는 것이 좋은 전략이라고 할 수 있겠네요. 프리트레이닝 단계가 아니라면 continual pretraining 전략도 생각해볼 수 있지 않을까 싶네요.

#multimodal #vision-language #instruction-tuning 