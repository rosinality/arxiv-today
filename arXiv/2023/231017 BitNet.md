https://arxiv.org/abs/2310.11453

BitNet: Scaling 1-bit Transformers for Large Language Models (Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei)

1 bit weight를 사용하는 트랜스포머. Quantization은 아니고 1 bit로 학습한다는 것을 전제하고 있습니다. Straight through estimator를 사용해서 학습시키는데 30B 까지 일단 학습이 되네요. 더 오래 학습시켰을 때의 결과가 궁금하긴 합니다.

scaling curve를 고려했을 때 동일 비트 기준의 효율성을 달성할 수 있을지가 포인트가 되겠군요.

#efficiency 