https://arxiv.org/abs/2309.02591

Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning (Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan)

메타발 multimodal generation 모델이군요. 특징적인 것은 1. retrieval을 사용해 추가 이미지-텍스트 페어를 입력 시퀀스에 결합. 2. infilling objective 사용 3. contrastive decoding 적용이 될 것 같군요.

추가적으로 sft 단계를 거쳐 text guided image editing 같은 과제를 수행할 수 있게 했습니다.

Gemini도 그렇고 multimodal input/output이 가능한 모델을 만드는 쪽으로 발전해나가고 있는 것 같은데 그런 모델들이 어떤 모양새일지 힌트가 되지 않나 싶네요. 다만 Midjourney 같은 경우 aesthetics를 많이 신경 쓴 결과라고 보이는데, 이런 multimodal 모델들이 aesthetic한 측면을 잘 커버해줄지 궁금하긴 합니다. 그것도 된다고 하면...너무 대기업의 횡포 아닐까요?

#multimodal #vision-language 