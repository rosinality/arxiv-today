https://arxiv.org/abs/2310.10477

Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis (Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu)

harmful한 응답을 생성하도록 프롬프트로 유도하고, 생성된 harmful한 응답이 왜 harmful한지에 대해 비평을 생성하게 한 다음, 응답과 비평을 결합해서 SFT를 하면 harmless한 응답을 생성하라고 했을 때의 성능이 나아진다는 결과군요. 일부러 좋지 않은 응답을 생성한다는 점에서는 RLCD가 생각나기도 하네요. (https://arxiv.org/abs/2307.12950)

비평을 생성하는 것이 왜 조건에 따라 생성하는 것에 도움이 되는가에 대한 설명이 있긴 합니다만, 여전히 신기하긴 하네요.

#alignment #safety

# Links

[[230724 RLCD.md]]
[[230330 Self-Refine]]