https://inflection.ai/inflection-2

Inflection의 두 번째 모델이 나왔군요. H100 5천 대, FP8 학습으로 50일 가량 학습했다고 하네요. 10^25 FLOPs 정도입니다.

성능적으로 세계 2위의 LLM이라는 자평에 걸맞게 PaLM 2에 대해 좀 더 우세를 보이고 있네요. 다만 10^25 FLOPs면 루머로 떠도는 PaLM 2의 학습 FLOPs의 수 배, GPT-4의 절반 정도의 규모인데 Llama 2, PaLM 2와 아주 큰 차이가 보이는 것 같지 않다는 게 걸리네요.

이건 Falcon-180B 때와 비슷한 현상인 것 같습니다.

왜 그런가...에 대해 생각해보게 되네요. 한 가지 가능성은 데이터셋의 퀄리티와 큐레이션 때문에 연산 투입량에 비해서 성능이 충분히 올라오지 않았다는 것, 다른 가능성은 PLM 자체만으로 벤치마크에서 성능을 향상시키는 것이 한계가 있고, Instruction Tuning을 거친 이후에야 PLM에 투입한 연산량에 따른 차이가 드러나기 시작할 수 있다는 것이 아닐까 싶습니다. 물론 둘 다 추측의 영역이긴 하죠.

별개로 FP8 학습이 이 정도 규모에서도 잘 작동한다는 것을 보여주는 사례가 될 것 같네요.

#llm 