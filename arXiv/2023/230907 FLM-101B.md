https://arxiv.org/abs/2309.03852

FLM-101B: An Open LLM and How to Train It with $100K Budget (Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang)

function preserving expansion (https://arxiv.org/abs/2305.02869, https://arxiv.org/abs/2308.06103) 을 사용해 llm을 학습해본 시도. 학습 효율화라고 하면 시도해보게 되는 방법 중 하나인데 지금까지 그렇게 성공적인 결과를 보여준 적은 없는 것 같네요

#efficient_training

# Links

# Links

