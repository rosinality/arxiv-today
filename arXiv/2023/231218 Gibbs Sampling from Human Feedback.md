https://arxiv.org/abs/2312.11456

Gibbs Sampling from Human Feedback: A Provable KL- constrained Framework for RLHF (Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, Tong Zhang)

Offline과 Online 세팅의 RLHF에 대한 이론적 분석. Pessimism/Uncertainty를 고려해야할 필요성에 대한 부분이 눈에 띄네요. DPO와 RSO에 대해서도 분석하는데 RSO의 문제 의식이기도 했던 Preference 샘플이 Optimal Policy가 아니라 Offline Policy에서 생성되는 것의 한계를 지적하네요. 여러모로 생각해볼만한 여지가 있지 않나 싶습니다. 하려고 하는 것이 Offline RL이라면 Offline RL에 대해서 먼저 생각해보는 것이 필요하지 않나 싶네요. (ReST에서 은근슬쩍 Offline RL 방법들을 끌어들인느 것처럼.)

#rlhf 