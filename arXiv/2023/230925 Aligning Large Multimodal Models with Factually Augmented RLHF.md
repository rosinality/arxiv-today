https://arxiv.org/abs/2309.14525

Aligning Large Multimodal Models with Factually Augmented RLHF (Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell)

Vision-Language 모델에서 할루시네이션을 억제하기 위한 RLHF 방법. 할루시네이션을 감지하기 위한 Reward Model이 필요한데, 일단 어노테이션 과정에서 할루시네이션을 찾아내도록 가이드를 했고, RM 입력으로 이미지와 이미지의 캡션을 사실에 대한 추가 정보를 제공하는 소스로 결합했네요.

할루시네이션이 발생한다는 것을 넘어 할루시네이션을 잡는 구체적인 방법이 중요한 상황에서 흥미로운 사례가 아닌가 싶습니다. Vision-Language 모델 뿐만 아니라 Language Only 사례에서도 이런 식으로 추가 사실 정보를 제공하는 방식을 고려할 수 있지 않을까 싶네요.

#vision-language #multimodal #alignment #hallucination 