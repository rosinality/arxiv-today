https://arxiv.org/abs/2308.03303

LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning (Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, Bo Li)

lora의 AB 행렬에서 dimension reduction을 수행하는 A를 얼려버리고 B만 학습시키면 어떨까. 학습 파라미터도 줄어들지만 low dimension인 XA activation만 저장하면 되니 activation memory를 크게 줄일 수 있다는 아이디어네요. 꽤 흥미롭습니다.

[[230711 Stack More Layers Differently]] ReLoRA와 결합되면 엄청난 효율성 향상이 가능할 수도 있지 않을까? 그러나 프리트레이닝 상황에서 weight 고정은 위험할 것 같음.

#efficient_training 