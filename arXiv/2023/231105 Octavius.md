https://arxiv.org/abs/2311.02684

Octavius: Mitigating Task Interference in MLLMs via MoE (Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Yu Qiao, Jing Shao)

이쪽도 어떻게 보면 비슷한 아이디어네요. 여러 modality과 과제를 통합하기 위해서 lora로 MoE 형태의 모델을 만들고, 각 instruction에 따라 특정한 expert lora를 선택하게 하는 방법. 포인트 클라우드 같은 입력도 시도해봤군요.

#multimodal #instruction-tuning #moe 