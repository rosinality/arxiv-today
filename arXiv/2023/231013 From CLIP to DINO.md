https://arxiv.org/abs/2310.08825

From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models (Dongsheng Jiang, Yuchen Liu, Songlin Liu, Xiaopeng Zhang, Jin Li, Hongkai Xiong, Qi Tian)

CLIP의 마지막 레이어의 feature만 따와서 multimodal 모델을 구성하는 경우가 많은데, 여러 레이어의 feature를 결합하는 것이 낫지 않을까, 또 DINO v2 같은 이미지 only 학습한 모델은 어떨까 하는 아이디어입니다. DINO v2가 꽤 좋은 성능을 보여주는 것이 흥미로운 부분인 것 같네요.

SLIP (https://arxiv.org/abs/2112.12750) 에서 image only contrastive learning이 image-text contrastive learning을 보완해줄 수 있다고 했던 것과 연결해서 볼 수 있지 않을까 싶습니다. MAE 같은 masked image modeling 방법에서 효과를 못 본 것도 비슷하군요.

#self_supervised #multimodal #contrastive_learning

# Links

# Links

