https://arxiv.org/abs/2310.10908

Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures? (Zihan Qiu, Zeyu Huang, Jie Fu)

Dense한 모델을 MoE 모델로 쪼개는 방법에 대한 연구군요. MoE를 Dense한 모델의 근사로 보는 연구 (https://arxiv.org/abs/2310.10837) 와 결합해서 생각한다면 흥미로운 지점이 있지 않나 싶습니다.

#moe

# Links

[[231016 Approximating Two-Layer Feedforward Networks for Efficient Transformers.md]]