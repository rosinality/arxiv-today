https://arxiv.org/abs/2310.02226

Think before you speak: Training Language Models With Pause Tokens (Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan)

모델이 다음 토큰을 예측하기 전에 pause 토큰을 삽입해 토큰 예측에 지연을 준 모델. 결과가 아주 강력한 것 같진 않긴 합니다만 모델이 쓸 수 있는 토큰을 임의로 추가한 논문들 (https://arxiv.org/abs/2309.16588, https://arxiv.org/abs/2309.17453) 을 보면서 했던 생각이 벌써 결과로 나온 셈이라 재미있네요.

#transformer #lm

# Links

[[230928 Vision Transformers Need Registers.md]]
[[230929 Efficient Streaming Language Models with Attention Sinks.md]]