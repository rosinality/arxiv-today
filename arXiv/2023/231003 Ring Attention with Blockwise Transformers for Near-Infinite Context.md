https://arxiv.org/abs/2310.01889

Ring Attention with Blockwise Transformers for Near-Infinite Context (Hao Liu, Matei Zaharia, Pieter Abbeel)

Blockwise Transformer (https://arxiv.org/abs/2305.19370) 에 Ring Self Attention (https://arxiv.org/abs/2105.13120) 을 결합했다는 느낌이군요. 기본적으로는 하드웨어에 올릴 수 있다가 포인트인 논문이긴 합니다만...시퀀스 길이 512K에서 Llama 13B를 튜닝한 결과가 툭 제시되어 있네요.

#efficient_training

# Links

[[230530 Blockwise Parallel Transformer for Long Context Large Models.md]]