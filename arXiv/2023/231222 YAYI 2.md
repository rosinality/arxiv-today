https://arxiv.org/abs/2312.14862

YAYI 2: Multilingual Open-Source Large Language Models (Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, Dajun Zeng)

중국 LLM 업계도 경쟁이 치열하네요. 30B / 2.65T 학습. 눈에 띄는 것은 천 개 단위의 휴리스틱으로 데이터 클리닝을 했다는 것, 데이터 비율에서 인터넷 크롤링 데이터가 50% 수준까지 낮아졌다는 부분이네요. 요즘 중국 LLM 모델이 종종 그렇듯 RLHF까지 거쳤습니다.

Contamination에 대한 언급이 없어서 신뢰하기 어렵지만 벤치마크 스코어가 굉장합니다. MMLU 80.5, HumanEval 53.1 등.

#llm #multilingual 