https://arxiv.org/abs/2311.02805

Tailoring Self-Rationalizers with Multi-Reward Distillation (Sahana Ramnath, Brihi Joshi, Skyler Hallinan, Ximing Lu, Liunian Harold Li, Aaron Chan, Jack Hessel, Yejin Choi, Xiang Ren)

답과 함께 타당한 근거를 제시할 수 있는 모델 만들기. 근거에 대해서 논리적이고 상식적인가(Plausibility), 명료하고 반복적이지 않은가(Diverse), 정답과 일관되는가(Consistent)라는 세 가지 타겟을 설정하고 이 타겟에 대해 RL을 한다는 접근이군요. 기본적으로 multi reward 문제가 되고, 여기서는 Quark를 사용해서 풀었습니다.

#alignment 

[[]]