https://arxiv.org/abs/2310.10505

ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models (Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo)

PPO 대신 REINFORCE (!)에 baseline으로 variance reduction을 해서 RLHF를 해보자는 아이디어군요. baseline으로는 프롬프트에 대해 추가로 뽑은 샘플에 대한 reward를 사용하는군요. Pairwise PPO (https://arxiv.org/abs/2310.00212) 와 꽤 비슷하지 않나? 하는 생각입니다.

#rl #alignment

# Links

[[230930 Pairwise Proximal Policy Optimization.md]]