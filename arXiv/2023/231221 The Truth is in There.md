https://arxiv.org/abs/2312.13558

The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction (Pratyusha Sharma, Jordan T. Ash, Dipendra Misra)

LLM의 Weight를 Low rank approximation하면 여러 벤치마크에서 성능이 향상된다는 결과. High order factor에 Low order factor와 충돌하는 사실들이 저장되어 있고, 그래서 그걸 삭제하면 성능이 더 나아질 수 있다고 제안하고 있네요. Regularization 같은 관점으로 보면 상당히 흥미로운 결과입니다만 이 과정에서 삭제되는 정보가 다른 과제나 OOD 상황에서 필요할 수 있지 않을까 하는 것이 문제겠군요. (https://x.com/AndrewLampinen/status/1739107003119837586)

#regularization #llm #transformer 