https://arxiv.org/abs/2309.11568

BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model (Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming (Charles)Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness)

cerebras의 BTLM-3B에 대한 리포트. SlimPajama를 사용했습니다. long context 부분이 눈에 띄었는데 약간 의아한 부분은 LongEval-Lines와 LongEval-Topics라는 두 retrieval 과제에 대해 MPT-7B, BTLM-3B, XGen-7B의 패턴이 많이 다르다는 것이네요. 플롯 legend를 잘못 넣었나 싶기도 하고...

그 외에도 lr decay 조정, SwiGLU vs GeLU, RoPE vs ALiBi 등에 대한 ablation이 있습니다. μP (https://arxiv.org/abs/2203.03466) 에서 별로 효과를 못 봤다고 했던 것 같은데 (https://arxiv.org/abs/2304.03208) 계속 쓰고 있네요.

#llm #long_context

# Links

# Links

