https://arxiv.org/abs/2312.03700

OneLLM: One Framework to Align All Modalities with Language (Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue)

이미지/오디오/포인트 클라우드 등에 대한 multimodal 모델. 각 데이터에 대해 1/2D Conv로 토크나이즈 한 다음 CLIP에 넣어버리고 MoE projection을 태워서 토큰으로 LLM에 넣는 방식이군요. 이렇게 합쳐나가다가 결국 인코더도 projection도 없이 트랜스포머 하나에 다 붙게 되겠죠. (어쩌면 이미 그렇게들 하고 있을 수도 있겠네요.)

#multimodal 