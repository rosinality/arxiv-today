https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf

Human-Centered Loss Functions (Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, Douwe Kiela)

응답 A > 응답 B 같은 preference가 아니라 개별 응답이 좋다/나쁘다를 사용해서 피드백을 줄 수 있을까? 사실 코드 실행 피드백 같이 Pass/Fail 피드백을 주는 경우에는 자연스러운 세팅이죠. Anthropic에서 Binary Discrimination vs Ranked Preference Modeling (https://arxiv.org/abs/2112.00861) 으로 테스트한 세팅이기도 하고요. 문제는 어떤 데이터가 자연스러운가 or 어노테이션에서 높은 퀄리티를 확보할 수 있는가인데 preference가 자연스러운 경우가 많지 않나 싶긴 하네요.

#alignment 
