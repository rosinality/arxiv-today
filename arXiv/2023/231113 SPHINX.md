https://arxiv.org/abs/2311.07575

SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models (Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao)

Vision-Language 모델들이 요즘 워낙 많이 나와서 이 모델들의 디테일을 정리하고 비교하는 것도 만만치 않네요.

요즘 나오는 결과들처럼 이 논문도 LM을 얼려놓고 학습하는 것에는 한계가 있다고 보고 일단 LM을 해동하고 학습을 진행했군요. 텍스트에 대한 지식이 날아가는 것은 텍스트 only 데이터를 사용하는 것으로 대응했습니다.

재미있는 포인트는 다른 도메인에서 튜닝한 weight를 weight mixing으로 결합한 것과, 서로 다른 visual encoder로 뽑은 임베딩을 결합해서 사용하는 것이네요.

이 논문도 입력 이미지의 해상도를 높이려고 시도하는데, 굉장히 공교롭게도 GPT-4V의 API 설명과 비슷하게 큰 이미지를 크롭한 이미지들과 전체 이미지를 저해상도로 리사이즈한 이미지에서 나온 토큰들을 결합해서 사용하는 군요.

#vision-language #multimodal 

[[231107 mPLUG-Owl2]]