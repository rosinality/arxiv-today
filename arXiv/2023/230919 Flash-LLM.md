https://arxiv.org/abs/2309.10285

Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity (Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon Song)

요즘 sparsity, 그것도 unstructured sparsity와 관련된 논문들이 나오고 있군요. sparse matrix으로 메모리 접근을 줄일 수 있다는 것을 사용해서, 메모리 사용량을 경감하고 연산은 tensor core로 dense matmul을 한다는 아이디어네요. llm의 다음 단계의 진전은 unstructured sparsity에서 오게 될까요? 궁금하네요.

#sparsity #efficiency 