https://arxiv.org/abs/2309.11235

OpenChat: Advancing Open-source Language Models with Mixed-Quality Data (Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu)

높은 퀄리티의 instruction 데이터 (GPT-4)와 낮은 퀄리티의 insturction 데이터 (GPT-3.5)가 섞여 있는 상황에서의 튜닝. 기본적으로 reward condition을 주고 학습하되, 추가적으로 loss를 reward로 reweighting 한다는 느낌이군요. 그렇다면 그냥 GPT-4 데이터만 쓴 것과 비교하면 어떤가 싶은데 그 비교는 없는 것 같네요.

#instruction 

[[231114 Adversarial Preference Optimization]]