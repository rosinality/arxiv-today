https://deepseekcoder.github.io/

주로 코드로 구성한 데이터 2T에 학습하고 Instruction Finetuning한 모델. 사실 성능보다 눈에 띈 것은 데이터를 구축한 방법. 가장 큰 특징은 코드 파일들 사이의 의존성을 분석한 다음 의존성에 따라 파일들을 재배치 했다는 것.

많이 나왔을 아이디어 같은데 (레포 내의 구조를 어떻게 반영할 것인가 같은 식으로) 실제 시도한 결과가 나온 것은 흥미롭다.

자연어에 대해서 생각해보면 In-Context Pretraining (https://arxiv.org/abs/2310.10638) 이 비슷한 아이디어일 듯 싶다. 유사한 텍스트들을 묶어서 정렬해서 학습하는 것. 관계가 있는 시퀀스들을 묶어서 처리하는 것으로 학습 효율성이나 모델의 특성을 개선할 수 있다는 건 꽤 그럴 듯한 아이디어인 것 같다.

모델 학습을 개선할 수 있을 것 같은 방법들이 지금도 여럿 눈에 띈다는 것이 재미있다. 지금보다도 더 발전된 모델이 마치 손에 잡히는 듯 느껴져서.