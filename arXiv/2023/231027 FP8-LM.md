https://arxiv.org/abs/2310.18313

FP8-LM: Training FP8 Large Language Models (Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, Peng Cheng)

FP8 학습 방법. Transformer Engine의 대안이라고 할 수 있겠네요. gradient allreduce와 ZeRO에서 scaling factor를 고려하는 것과 low precision optimizer가 메인 포인트군요. FP16과 성능이 동등하다고 이야기하고 있는데 대규모로, 끝까지 학습했을 때에도 성능 차이가 없는지 궁금하긴 합니다.

#efficient_training 