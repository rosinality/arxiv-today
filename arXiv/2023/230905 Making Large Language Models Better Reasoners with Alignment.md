https://arxiv.org/abs/2309.02144

Making Large Language Models Better Reasoners with Alignment (Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui)

cot로 그냥 파인튜닝을 하면 잘못된 cot에 대해 perplexity를 낮게 부여하는 경우가 생기고 이것이 문제다, 그러니 올바른 cot와 잘못된 cot 사이에 contrastive loss를 붙여보자...는 아이디어입니다. 딱 봐도 openai가 rl로 풀고 싶어하는 (혹은 풀고 있는) 문제로 보이네요.

#reasoning #prompt 