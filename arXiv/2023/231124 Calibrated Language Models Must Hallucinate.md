https://arxiv.org/abs/2311.14648

Calibrated Language Models Must Hallucinate (Adam Tauman Kalai, Santosh S. Vempala)

캘리브레이션이 되어있는 LM이 학습 데이터에 없는 사실을 생성(할루시네이션)할 확률은 그 사실과 관련된 사실들이 전체 데이터에서 단 한 번 등장한 비율에 의해 결정된다는 분석. 그러니까 학습 데이터에서 단 한 번만 등장하는 형태의 사실들이라면 (인용을 생성한다고 하면 데이터에서 단 한 번만 인용된 사례가 많다면) 할루시네이션이 발생할 수밖에 없다는 것이네요.

할루시네이션의 발생율은 캘리브레이션의 정도에 따라 바뀌기 때문에 할루시네이션을 억제하려면 캘리브레이션의 정도가 낮아질 수밖에 없다는 것도 시사하긴 합니다.

그렇지만 다르게 말하면 학습 데이터에 한 번 이상 등장하는 사실들에 대해서는 할루시네이션이 드물 것이라는 예측이기도 합니다. 그런데 실제로 그렇지 않다면 그건 모델의 Capacity의 문제에서 기인한 것이라고 볼 수 있지 않을까 제안하고 있네요.

#lm #hallucination 