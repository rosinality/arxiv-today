https://arxiv.org/abs/2310.17157

Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time (Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen)

FFN의 뉴런이나 Attention 헤드가 희소하다고 보고 이걸 사용해 FFN, Attention의 일부만 계산하자는 아이디어. 그러나 입력과 관계 없이 Pruning을 하는 것은 아니고 입력에 따라 쓰일 헤드나 뉴런을 찾는 MLP를 붙여서 하는 접근입니다.

배치 추론에는 잘 안 맞겠지만 흥미롭네요. (이 논문에서도 OPT를 썼는데) ReLU의 희소함을 이용할 수 있지 않을까라는 연구도 생각나네요. (https://arxiv.org/abs/2310.04564) 그리고 참 MoE스러운 접근이다 싶은데 이런 sparse 모델들에 대한 관심이 계속 높아지이 않을까 싶습니다.

#sparsity #efficiency

# Links

# Links

