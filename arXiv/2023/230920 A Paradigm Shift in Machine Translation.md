https://arxiv.org/abs/2309.11674

A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models (Haoran Xu, Young Jin Kim, Amr Sharaf, Hany Hassan Awadalla)

프리트레이닝된 LM에 Llama에 부족한 언어의 코퍼스를 monolingual하게 학습시키고, 소량의 병렬 코퍼스를 학습시키면 SoTA를 깨는 성능을 낼 수 있다는 결과. 병렬 코퍼스가 많으면 프리트레이닝에서 습득한 것이 사라질 뿐이니 많을 필요는 없고 품질이 더 중요하다고 보고 있네요. 다만 데이터셋 오염에서 자유로울까? 하는 생각은 있습니다.

#nmt 

