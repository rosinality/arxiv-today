https://arxiv.org/abs/2311.11829

System 2 Attention (is something you might need too) (Jason Weston, Sainbayar Sukhbaatar)

LM이 잘못된, 혹은 불필요한 맥락 정보가 주어졌을 때 잘못된 텍스트를 생성하는 경향이 있고, 이에 대해 불필요한 정보를 걷어내게 하는 프롬프트로 맥락 텍스트를 다시 생성한 다음 이 텍스트로 추론했을 때 성능이 향상된다는 결과. 이 방법으로 아첨을 줄일 수 있었다고 하네. (https://arxiv.org/abs/2310.13548) 노트 정리를 시키거나 (https://arxiv.org/abs/2311.09210) negative 샘플에 대한 내성을 키우는 것 (https://arxiv.org/abs/2311.09198) 과 비슷한 선상에 있는 아이디어가 아닐까 싶습니다.

#prompt

# Links

[[231020 Towards Understanding Sycophancy in Language Models.md]]
[[231115 Chain-of-Note.md]]
[[231115 Never Lost in the Middle.md]]