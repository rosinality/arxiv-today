https://arxiv.org/abs/2309.00267

RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi)

summarization에 대해 feedback from llm을 시도했군요. summarization에 특화된 preamble을 사용하긴 했습니다만 constitutional ai의 사례처럼 다양한 과제에 대한 constitution을 만드는 것이 가능하지 않을까 싶기도 하네요.

다만 답답한 것 중 하나는 모델 테스트를 PaLM 2에 대해서 했다 보니 실험 모델에 대해서는 PaLM 2 Extra-Small 이라고만 되어 있습니다. 대체 어느 정도의 모델인지 알 수가 없네요.

#alignment 