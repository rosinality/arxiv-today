https://arxiv.org/abs/2312.06585

Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models (Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, Noah Fiedel)

네...모두가 예상하고 기대하던 방향의 결과가 나왔군요. ReST (https://arxiv.org/abs/2308.08998) 기반으로 바이너리 리워드를 줄 수 있는 MATH, APPS에 대하 튜닝해본 시도입니다. 여러 이터레이션을 돌릴 수 있는 것은 아닌데 (이터레이션에 따라 성능이 감소합니다.) 사람이 작성한 솔루션으로 튜닝하는 것보다 성능이 더 나을 수 있다는 것을 보였습니다. 헝가리 고교 수학 시험이나 Big Bench 과제들에 대한 Transfer도 가능했군요. 이런 형태로 리워드를 줄 수 있는 과제들을 많이 세팅하면 모델의 전반적인 추론 능력 향상 등을 기대할 수 있다는 증거로 보이네요.

서치와 RL로 인간을 뛰어넘는 퍼포먼스를 낼 수 있지만 역시 문제는 리워드죠.

#synthetic-data #rl

# Links

[[230817 Reinforced Self-Training (ReST) for Language Modeling.md]]