https://arxiv.org/abs/2309.10818

SlimPajama-DC: Understanding Data Combinations for LLM Training (Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric Xing)

프리트레이닝 코퍼스의 여러 믹스를 만들어서(SlimPajama, GitHub, Books, Wikipedia, RefinedWeb) 프리트레이닝 한 다음 벤치마크를 해봤군요. 1.3B 모델에서 나온 결과이긴 하고 벤치마크 결과가 혼란스럽긴 합니다만 여러 데이터를 섞는 게 좋긴 한 건가? 하는 느낌과 RefinedWeb (https://arxiv.org/abs/2306.01116) 의 퀄리티에 대한 말이 좀 있는데 여기서는 그럭저럭 괜찮아 보이네? 하는 느낌입니다.

jusText (The Pile), Trafilatura (RefinedWeb) 같은 추출기를 사용하는 것이 나은가 WET을 쓰는 것이 나은가, 모델 기반 퀄리티 필터링이 나은가 Gopher 스타일의 휴리스틱 기반 필터링이 나은가 등등...궁금한 부분이 많네요.

#dataset

# Links

# Links

