https://arxiv.org/abs/2312.00968

Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts (Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, Radu Soricut)

프리트레이닝된 모델에 Soft MoE (https://arxiv.org/abs/2308.00951) 기반 expert를 추가한다는 아이디어군요. Multimodal 시나리오에서 모델을 얼려놓는 것으로는 한계가 있다는 요즘 결과들 (https://arxiv.org/abs/2311.03079, https://arxiv.org/abs/2311.04257) 과 조합해서 볼 수 있을 것 같네요.

#multimodal #vision-language 

[[230802 From Sparse to Soft Mixtures of Experts]]
[[231106 CogVLM]]
[[231107 mPLUG-Owl2]]

# Links

[[230802 From Sparse to Soft Mixtures of Experts.md]]
[[231106 CogVLM.md]]