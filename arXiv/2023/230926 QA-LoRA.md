https://arxiv.org/abs/2309.14717

QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models (Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, Qi Tian)

QLoRA처럼 Quantization이 된 Weight를 사용하는 것을 넘어, 튜닝 결과물도 Quantization이 되어있는 상태였으면 좋겠다는 아이디어에서 나온 결과. Groupwise Quantzation을 사용하고, 그에 대응해 같은 그룹 내의 LoRA Weight는 공유되도록 만들어서 학습. 흥미롭네요. 다만 나온 결과 수치들이 Llama의 결과와는 꽤 달라서 어떻게 평가해야 할지는 좀 어렵습니다.

#efficient_training #quantization 