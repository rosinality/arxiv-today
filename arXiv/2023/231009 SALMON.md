https://arxiv.org/abs/2310.05910

SALMON: Self-Alignment with Principle-Following Reward Models (Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan)

Dromedary (https://arxiv.org/abs/2305.03047) 의 버전 2가 나왔네요. 이쪽은 RLAIF 파이프라인을 구성하는데 중점을 뒀습니다. 기본적인 접근은 Dromedary와 같이 원칙(Principle) 기반입니다. 원칙을 사용해서 preference comparison과 reward model을 만들어서 RL 과정에 사용하는 것이죠. 추가적으로 reward hacking이 발생하는 것을 관찰했고, reward hacking에 대응하기 위한 principle도 작업 과정에서 작성해서 추가했네요.

흥미롭습니다. 긴 텍스트가 선호되는 점을 노리고 길이에 따른 보너스를 줬다는 부분 등이 좀 걸리긴 합니다만 한 번 결과 모델을 살펴보고 싶네요.

#alignment

# Links

[[230504 Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision.md]]