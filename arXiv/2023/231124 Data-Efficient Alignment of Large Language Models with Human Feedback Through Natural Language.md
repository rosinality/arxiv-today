https://arxiv.org/abs/2311.14543

Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language (Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Padmakumar, Sungjin Lee, Yang Liu, Mahdi Namazifar)

1000개의 프롬프트, 응답, 비평, 개선된 응답 데이터를 구축한 다음 이걸로 모델을 튜닝해서 효과를 봤네요. 비평을 생성하게 했을 때 성능 향상이 있었다, 다른 모델의 응답을 개선하는데 쓸 수 있었다, 반복적으로 적용해서 응답의 성능을 개선할 수 있었다는 결과입니다.

#llm #instruction-tuning 

[[230330 Self-Refine]]