https://huggingface.co/blog/falcon-180b

Falcon 180B. 3.5T 토큰으로 학습됐습니다. 공개된 LLM 중 가장 대규모의 모델이네요. 주장하기로는 PaLM 2 Large에 근접하는 성능이라고 합니다. 다만 커뮤니티 분위기는 어? 하는 느낌이 있네요. 180B 규모라 오픈소스 커뮤니티에서 다룰만한 규모는 아닌데 (4 bit quantization을 하더라도) LLM Leaderboard 스코어는(물론 이건 단 4개의 벤치마크를 평균낸 것이지만) Llama 2 70B보다 1점 정도 높은 수준입니다.

PaLM 2가 340B 파라미터에 3.6T 토큰이라고 하니 비슷한 성능이라고 하면 합리적인 것 같긴 한데 (물론 Falcon 180B보다 PaLM 2가 "더" multilingual 하긴 합니다.) Llama 2 70B 파라미터 & 2T 토큰에 비해 거의 5배의 FLOPS를 썼다는 점에서 보면 좀 아쉬운 것이 아닌가 싶기도 하고 그렇습니다.

정보를 줍는 것을 좋아하는 입장에서 Chinchilla를 넘어서는 규모의 모델 학습에 대한 정보가 없다는 느낌이 들긴 하네요. Llama 2만 해도 추가적인 600B 토큰의 구성이 어떻게 되는지 알려주지 않았죠.

#llm 