https://arxiv.org/abs/2309.09530

Adapting Large Language Models via Reading Comprehension (Daixuan Cheng, Shaohan Huang, Furu Wei)

llm의 domain adaptation/finetuning에서 그냥 프리트레이닝을 더 하는 방식이 아니라(continued pretraining) synthetic하게 만든 instruction 형태의 텍스트들(reading comprehension)과 general instruction data를 결합해서 튜닝했을 때 더 나은 결과를 얻었다는 결과. instruction tuning이 갖는 벤치마크에서의 장점을 고려하긴 해야겠습니다만 Textbooks Are All You Need (https://arxiv.org/abs/2306.11644) 도 그렇고 이런 형태의 데이터가 갖는 의미를 생각해볼 가치는 있을 것 같네요.

#domain_adaptation #finetuning #dataset

# Links

[[230620 Textbooks Are All You Need.md]]
[[230928 Qwen Technical Report]]