https://arxiv.org/abs/2311.08105

DiLoCo: Distributed Low-Communication Training of Language Models (Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc'Aurelio Ranzato, Arthur Szlam, Jiajun Shen)

써야 하는 GPU 규모가 늘어나면서 하나의 클러스터에서 synchronous하게 학습하는 것이 더 어려워지고 있으니, federated learning에서 아이디어를 따와서 하나의 클러스터 내에서 학습한 다음 클러스터 간에서 그래디언트를 합쳐 최적화한다는 아이디어군요. 결과적으로 통신 요구량도 크게 줄어들고 클러스터 간 대역폭이 작은 경우에도 좀 더 할만해지죠.

그래서 성능은 잘 보존되나? 라고 하면 잘 보존되는 것처럼 보이네요. 400M 정도의 모델이긴 하지만요. 논문에서는 모델이 더 커지면 더 잘 될 것이라고 추측하고 있는데 실제로 그럴지 궁금하네요.

#efficient_training 