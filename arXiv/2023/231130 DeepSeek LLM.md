https://github.com/deepseek-ai/deepseek-LLM

DeepSeek Coder (https://github.com/deepseek-ai/DeepSeek-Coder) 를 냈던 DeepSeek에서 자연어 LLM을 냈군요. 67B, 2T 토큰 학습, 영어와 중국어 Bilingual 모델인데 전반적 성능, 특히 수학과 코딩에서 성능이 높네요.

프리트레이닝을 어떻게 했는가에 대해서 간략한 소개가 있긴 한데 구체적인 정보는 없긴 합니다. 다만 이 트윗의 리플에서 나오는 이야기처럼 (https://x.com/ocolegro/status/1729883964037644614) 고품질의 데이터를 확보하는 과정에서 다국어 데이터가 도움이 될 수 있다는 생각이 드네요. 영어가 데이터가 많긴 하지만 어쨌든 사용 토큰이 늘어날 수록 퀄리티 측면에서 꼬리에 있는 데이터들이 더 포함되게 될 테니까요.

#llm 