https://arxiv.org/abs/2310.14189

Improved Techniques for Training Consistency Models (Yang Song, Prafulla Dhariwal)

Consistency Models에서 Distillation이 없는 Consistency Training에 대한 개선이군요. Teacher EMA를 없애고 LPIPS 대신 Huber Loss 계통의 Loss로 바꿨군요. 바닥부터 학습 가능한 고속 샘플링 모델로서 성능 향상까지 있어서 꽤 유의미한 결과가 아닐까 싶네요.

#ddpm 

[[230302 Consistency Models]]