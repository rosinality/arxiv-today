https://arxiv.org/abs/2308.12284

D4: Improving LLM Pretraining via Document De-Duplication and Diversification (Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos)

메타에서 프리트레이닝 데이터셋 샘플링 관련 결과를 냈군요. fuzzy dedup 위에 추가적인 dedup과 클러스터링을 사용한 diversification 입니다. 같은 토큰 수로 학습시켰을 때 샘플링된 데이터셋의 퍼포먼스가 더 낫고, 데이터 양을 고정하고 샘플링으로 2 epoch 학습을 시키는 시나리오에서도 전체 데이터셋으로 1 epoch 학습하는 것보다 나은 결과를 냈군요.

다만 웹 데이터셋에서 이슈가 발생하네요.

여담이지만 llama가 fuzzy dedup을 안 했다는 증거가 하나 추가됐네요.

#pretraining 

[[210714 Deduplicating Training Data Makes Language Models Better]]
[[230719 Llama 2]]