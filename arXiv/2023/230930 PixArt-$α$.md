https://arxiv.org/abs/2310.00426

PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis (Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie1, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li)

Diffusion Transformer (https://arxiv.org/abs/2212.09748) 를 기반으로 학습 파이프라인과 데이터셋을 개량한 방법이군요. 일단 이미지넷에 클래스 컨디션으로 학습시키고, SAM (https://arxiv.org/abs/2304.02643) 의 높은 퀄리티의 데이터셋에 LLaVA를 사용해 캡션을 달아서 이미지-텍스트 데이터셋을 구축하고, 다시 높은 퀄리티의 자체 데이터셋에 대해 튜닝하는 흐름입니다. 필요 데이터셋과 연산량의 감소에 포커스를 맞추고 있는데 워낙 조건이 달라서 비교하기가 쉽지는 않네요.

다만 높은 퀄리티의 이미지를 선정하고 거기에 캡션 모델로 이미지-텍스트 페어를 만든다는 흐름 자체는 눈에 띄네요.

#ddpm #text2img

# Links

[[231020 DALL-E 3]]