https://arxiv.org/abs/2312.14238

InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks (Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai)

https://github.com/OpenGVLab/InternVL

ViT 6B와 Llama 7B를 묶어서 CLIP으로 프리트레이닝, 이 두 모델을 Cross Attention으로 묶은 다음 Generative/Constrastive/Matching Loss로 학습, 그 위에 13B 디코더를 붙여서 Instruction Tuning 순서로 구축한 모델이군요.

#multimodal #vision-language #clip #instruction-tuning 