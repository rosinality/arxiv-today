https://platform.openai.com/docs/guides/vision
https://openai.com/pricing

GPT-4 Vision의 이미지 처리 방법이 꽤 특이한 듯 싶다. 일단 고해상도와 저해상도 모드가 있는데 저해상도 모드에서는 512x512px로 처리해서 85 (혹은 65) 토큰을 만든다. 문서 내에서 토큰 수로 제시하고 있는 값이 다르다.

고해상도 모드에서는 일단 장축이 2048px 이하가 되도록 리사이즈 한 다음 단축이 768px가 되도록 다시 리사이즈한다. 그리고 리사이즈한 이미지를 512x512px 크롭으로 쪼갠다. ceil((width or height) / 512) 개의 크롭을 만들기 때문에 크롭은 서로 겹칠 수도 있고 아닐 수도 있는 것으로 보인다. 각 크롭에 대해서 170 (혹은 129) 토큰을 만든다. 여기에 저해상도 모드의 토큰(85 or 65)을 더한다.

대략 2048x768px 이미지 정도가 최대 크기가 될 것 같고, 그랬을 때 512x512px 크롭은 8개가 나온다. 꽤 고해상도로 처리한다고 봐야할 듯 싶다. 저해상도 이미지로 전역적인 정보를 추가로 사용한다는 것은 이해할 수 있는데 크롭을 쓰는 이유는 잘 모르겠다. 시퀀스 토큰 수를 줄여 attention의 비용을 줄이기 위함일지. 거기에 overlapping window도 아니고 이미지 크기에 따라 overlapping 할 수도 아닐 수도 있다는 것도 좀 특이한 것 같고.

거기다 512px 이미지에 대해서 85, 175 같은 토큰 수가 나오는 것도 좀 특이하다. 고해상도 모드의 175 토큰은 저해상도 모드의 85 토큰의 두 배로 맞춰져 있다는 것도 묘한 것 같고. 단순하게 생각하면 85 토큰에서는 패치 크기가 56px 정도이고 170 토큰에서는 39px 정도 될까?

물론 공식적으로 비용 계산을 위해 밝힌 방식이 이런 것일 뿐이고 실제로 돌아가는 방식은 다를 수도 있긴 하겠다. 여하간 이런 형태의 계산법이 가능한 아키텍처가 무엇일지.