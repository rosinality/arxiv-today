https://arxiv.org/abs/2310.10837

Approximating Two-Layer Feedforward Networks for Efficient Transformers (Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber)

MoE를 MLP에 대한 근사로 보고, 반대로 MLP를 더 잘 근사하는 MoE를 디자인해본다는 아이디어군요. Transformer-XL을 WikiText-103에 대해 학습한다는 좀 이전의 세팅이긴 합니다. MoE를 채택한다고 하면 MoE 디자인에 대해 탐색하는 것은 의미가 있을 듯 싶긴 하네요.

#moe 