https://arxiv.org/abs/2309.10400

PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training (Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li)

long context 파인튜닝시에 long context input을 바로 사용하는 것이 아니라, 짧은 청크에 대해 position id를 조정해서 position id가 연속적으로 증가하는 것이 아니라 건너뛰게 만드는 방식(즉 1024에서 1025가 아니라 1536으로 넘어가는 식으로)으로 다양한 position에 대해 학습시키겠다는 아이디어. 재미있네요. 그냥 바로 파인튜닝하는 성능에는 못 미치는 것 같긴 합니다만.

#transformer 