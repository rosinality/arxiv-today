https://arxiv.org/abs/2309.04827

Neurons in Large Language Models: Dead, N-gram, Positional (Elena Voita, Javier Ferrando, Christoforos Nalmpantis)

트랜스포머 FFN 열어보기.

1. 초기 레이어에서 대부분의 뉴런이 activate 되지 않는다. 낮은 단계에서는 저수준의 처리가 일어나기 때문에 패턴 표현에 특별히 많은 뉴런이 필요하지 않다는 추측. 높은 단계에서는 반대로 수많은 패턴을 커버해야 하는 필요가 생긴다는 것이죠. Sandwich Transformer (https://arxiv.org/abs/1911.03864) 가 생각나는 군요.
2. 초기 레이어에서, 모델이 커질 수록 n-그램 검출기 뉴런이 많아진다. n-그램 검출기 뉴런이 많아진다는 것은 특정(1~5개 정도의) 토큰에 대해서만 반응하는 뉴런이라는 의미입니다. 거기에 여러 FFN의 뉴런들이 각기 다른 n-그램에 반응해서 종합적으로 n-그램 검출기를 구성하게 되네요.
3. 이러한 n-그램 검출기는 결과적으로 n-그램이 감지되었을 때 예측 토큰의 확률을 변화시키는 것인데(https://arxiv.org/abs/2203.14680), 관련 있는 토큰의 확률을 높이는 것이야 예상 가능한 부분이지만 의외로 검출된 토큰 자신의 확률을 낮추는 경향이 발생하는 군요. 즉 Weather라는 토큰이 검출되었다면 Weather 토큰의 확률을 낮춥니다.
4. 특정한 위치에 반응하는 뉴런들. Position에 대해 특정한 주기로 특정 위치에만 반응하는 진동하는 경우, 특정 역치에 따라서 역치를 넘으면 켜졌다가 역치 아래에서는 켜지지 않는 경우, 특정 위치를 넘으면 반응을 하거나 반응을 하지 않는 경우, 그 외 위치에 따라 패턴이 있는 경우 등등이 있습니다.
5. 이것도 모델 크기에 따라 패턴이 달라집니다. 작은 모델에서는 구체적인 위치를 탐지하는 뉴런이 많고 큰 모델에서는 위치에 따른 경향적인 패턴을 보이는 뉴런이 많습니다. 여하간 더 큰 모델일수록 구체적인 위치가 아니라 보다 추상적인 위치 관계에 반응하는 것으로 보입니다.
6. 그리고 이 위치 검출 뉴런도 레이어별로 연계되어 있어서 Redundancy 없이 전체 위치 범위를 커버하도록 구성되어 있네요.
7. 위치 검출 뉴런도 레이어 위치에 따라 분포가 달라집니다. 하위 레이어에서는 구체적인 위치 검출을 통해 낮은 단계의 패턴을 검출한다면 상위 레이어로 갈수록 더 높은 단계의 패턴 검출을 위한 위치 검출 뉴런의 분포가 많아진다는 이야기를 하는 군요.
8. 그런데 이건 모두 Absolute Position Embedding을 사용하는 OPT 기반 분석이죠. 그래서 Position Embedding을 빼고 학습시켜 봤습니다. 그래도 여전히 위치 검출 뉴런들이 발생하긴 하지만 어떤 종류의 위치 검출을 하는 뉴런이 많은가에 대한 패턴은 달라지는 군요. Absolute Position Embedding의 경우 진동하는 뉴런이 많고 Position Embedding은 역치에 따라 움직이는 뉴런들과 위치에 따른 경향성 정도를 보이는 뉴런 많습니다.
9. 이런 위치 검출 뉴런 때문에 FFN이 특정 텍스트 패턴에 반응해 출력 분포를 업데이트한다는 설명에 한계가 있지 않은가 하는 이야기를 합니다. 텍스트 내용과는 별개로 위치에만 반응하는 뉴런이 있다는 건 특정 텍스트-위치 패턴에 반응해서 움직인다는 설명으로는 바로 설명하기 어렵죠.
10. 그런데 350M 모델은 또 이와는 다른 패턴을 보입니다. 350M 모델의 차이는 Post LN을 썼다는 것인데 그게 모델의 거동을 상당히 바꿨다는 의미가 되는 군요. (그럴 수 있을 것 같긴 하네요.) 거기다 OPT는 ReLU를 쓰기 때문에...다른 Activation에 대해서는 패턴이 또 달라질 수 있겠죠.

여하간 이런 트랜스포머 분석 논문을 보면 토큰 패턴을 검출해서 출력 분포를 업데이트하는 것 뿐이네라고 말할 수 있을 것 같다가도 꼭 그것만으로는 설명할 수 없는 것 같기도 하고 그렇네요. 그렇지만 여전히 큰 모델은 다르다는 건 이런 방식으로도 확인되는 것 같습니다.

#transformer

# Links

# Links

