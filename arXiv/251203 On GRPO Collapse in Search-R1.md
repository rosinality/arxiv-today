https://arxiv.org/abs/2512.04220

*On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral* (Wenlong Deng, Yushu Li, Boying Gong, Yi Ren, Christos Thrampoulidis, Xiaoxiao Li)

> Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

특히 도구 사용 RL의 경우에 올바른 액션에 대한 Likelihood의 업데이트가 매우 작거나 음수일 수 있다고. 이건 아마도 도구 출력 토큰이 모델에게 OOD에 가까워서 불확실성을 높이고, 또한 구조적으로 궤적 사이에 공유되는 Prefix 때문일 수 있지 않을까 하는 설명.

The update for the likelihood of correct actions can be very small or even negative, especially in RL with tool use. This might be because of rather OOD tokens from tool outputs and increased uncertainty from it, and structurally shared prefixes among trajectories.

#rl #agent 