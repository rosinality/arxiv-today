https://arxiv.org/abs/2510.14807

*SimKO: Simple Pass@K Policy Optimization* (Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen)

> Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.

엔트로피가 높은 (Branching) 토큰의 top-K Vocabulary에 대해 Label Smoothing을 적용하는 것으로 pass@K 성능 향상. 어쩌면 최근 나오는 top-P Vocabulary에 대해 엔트로피 보너스를 적용하는 것과 비슷한 효과일지도. 잘 작동할 수는 있지만 여전히 휴리스틱이라는 느낌을 받음. 좀 더 원칙에 기반한 접근이 존재할지.

Improving pass@K performance by applying label smoothing to top-K vocabularies for high entropy (branching) tokens. This might have a similar effect to recent approaches that apply entropy bonuses to top-P vocabularies. While it could work well, I feel this is still a heuristic. Would there be a more principled approach?

#rl #reasoning 