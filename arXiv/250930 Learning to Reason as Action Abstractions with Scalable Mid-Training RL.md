https://arxiv.org/abs/2509.25810

*Learning to Reason as Action Abstractions with Scalable Mid-Training RL* (Shenao Zhang, Donghan Yu, Yihao Feng, Bowen Jin, Zhaoran Wang, John Peebles, Zirui Wang)

> Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.

아주 흥미로운 연구. 기본적으로 나머지 시퀀스에 대한 Likelihood를 보상으로 사용해 미드트레이닝을 위한 코퍼스에 추론을 생성하는 연구. 그러나 여기에서는 불필요한 추론 단계에 페널티를 가해 추론이 반드시 도움이 되는 경우에만 삽입하도록 유도.

This is very interesting! Basically synthesizes rationale for the mid-training corpus using the likelihood of the remaining sequences as a reward. But this work penalizes unnecessary reasoning steps thus inserting rationale only when it is beneficial.

#synthetic-data #mid-training #rl #reasoning 