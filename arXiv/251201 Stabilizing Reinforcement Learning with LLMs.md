https://arxiv.org/abs/2512.01374

*Stabilizing Reinforcement Learning with LLMs: Formulation and Practices* (Chujie Zheng, Kai Dang, Bowen Yu, Mingze Li, Huiqiang Jiang, Junrong Lin, Yuqiong Liu, An Yang, Jingren Zhou, Junyang Lin)

> This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.

RL 안정화. 토큰 단위 Objective를 시퀀스 단위 Objective의 근사로 간주. 그러나 이 근사가 성립하기 위해서는 타겟 Policy와 롤아웃 Policy의 Importance Ratio가 작아야 함. 이 가정 위에서 Objective를 디자인하고 라우팅 리플레이 전략을 비교함. 어쩌면 아주 안정적인 RL 알고리즘이 곧 등장할지도?

Stabilizing RL. Consider token level objective as an approximation to sequence level objective. But for this approximation to hold the importance ratio between target policy and rollout policy should be small. They built the objective on top of this and compared routing replay strategies. Maybe we can get a super stable RL algorithm soon?

#rl 