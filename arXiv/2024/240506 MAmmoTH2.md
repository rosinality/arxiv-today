https://arxiv.org/abs/2405.03548

*MAmmoTH2: Scaling Instructions from the Web* (Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen)

> Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 34% on MATH and from 36% to 67% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.

QA 형태의 문서를 웹 크롤에서 발굴한 다음 LLM으로 QA 페어를 추출하고 Refine해서 Instruction 데이터로 사용한 방법. 여기서는 Instruction Tuning을 위한 데이터로 사용했는데 프리트레이닝 시점에 사용할 대규모 고품질 데이터로서는 어떨까 싶기도 하네요. (https://arxiv.org/abs/2401.16380)

#instruction-tuning #synthetic-data

# Links

[[240129 Rephrasing the Web.md]]