https://arxiv.org/abs/2402.00159

*Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research* (Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo)

> Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.

이전에 공개됐었던 Dolma (https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64) 데이터셋에 대한 리포트. 데이터셋에 대해 이렇게 상세하게 이야기하는 사례가 이제 극히 드물다는 것을 고려하면 흥미롭습니다.

블로그에서도 나와 있었던 부분이지만 CCNet 기반으로 모델 기반 필터링 없이 Gopher의 휴리스틱과 C4의 구두점으로 끝나지 않는 문단을 날리는 필터링을 하고 컨텐츠 필터링, URL/문서/문단 레벨 Exact Deduplication을 했습니다. 사실 이 개별 선택들이 약간씩 특이한 점이 있는데 그에 대한 일부 근거들이 제시되어 있습니다.

CCNet vs Trafilatura 같은 Extractor의 비교, 모델 기반 필터링 vs 휴리스틱, Fuzzy Deduplication 등등이 궁금하긴 합니다만 이건 직접 해보는 수밖에 없겠군요.

별개의 이야기지만 웹 이외의 데이터셋을 보다 대규모로 확충하는 것이 필요하다는 생각도 있습니다. 어렵지만요.

#dataset #corpus 

Techinical report Dolma corpus which is previously announced. As it is very rare to show about dataset construction recently, it is quite interesting.

They applied CCNet, and used heuristics of Gopher and C4 which removes paragraph does not ends with punctuations, as stated in the blog. Also they have used content filtering, URL/Document/Paragraph level exact deduplication. Actually these choices are all slightly unusual, and some supporting evidences for it suggested.

I am curious about comparisons between CCNet vs Trafilatura-like extractors, Model based filtering vs Heuristics, Fuzzy Deduplications. But there are no other way besides do it myself.

Besids, I think we should construct large datasets that does not from Webs. But of course it will be hard.