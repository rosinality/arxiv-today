https://arxiv.org/abs/2402.04617

*InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory* (Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, Maosong Sun)

> Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining the ability to capture long-distance dependencies. Without any training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies.

LM-Infinite (https://arxiv.org/abs/2308.16137) 에서 초기 토큰과 Local Window로 Attention Mask를 만들어 Long Context 문제에 대응하는 아이디어에 덧붙여 초기 토큰과 Local Window 토큰들 사이의 토큰들을 의미 있는 토큰으로 채워보자는 아이디어.

KV Cache를 쪼개고 쪼갠 각 청크마다 대표 토큰 임베딩을 만들어서 현 토큰과 관련도가 높은 청크를 끼워넣는 방식입니다. 즉 초기 토큰, 관련도가 높은 토큰들, 근방의 토큰들 순서로 KV Cache가 구성되는 것이죠.

LM 내에서 RAG를 하는 것과 비슷하고 이런 식으로 관련된 상태를 Retrieval 하자는 아이디어는 종종 나왔었죠. (https://arxiv.org/abs/2305.01625) 재미있네요.

#long-context

# Links

[[230502 Unlimiformer.md]]