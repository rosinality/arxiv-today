https://arxiv.org/abs/2401.08740

*SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers* (Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, Saining Xie)

> We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.

Stochastic Interpolant 프레임워크 기반으로 Diffusion 모델을 보다 유연하게 만들어서 Continuous Time, Score 대신 Velocity를 사용, 데이터와 노이즈 분포에 대한 다양한 Interpolant, Diffusion Coefficient에 대한 변경 등등을 가능하게 만든 다음 이 세팅들에 대한 비교를 한 결과입니다. Stochastic Interpolant를 선행적으로 봐야할 듯 한데 이것 자체가 만만치 않군요. (https://arxiv.org/abs/2303.08797)

#diffusion

# Links

