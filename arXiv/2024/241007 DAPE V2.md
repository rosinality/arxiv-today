https://arxiv.org/abs/2410.04798

*DAPE V2: Process Attention Score as Feature Map for Length Extrapolation* (Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li)

> The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens, in contrast to earlier feed-forward neural networks. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem. The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.

이건 상당히 신기하네요. Attention Score에 Convolution을 한 결과를 Attention Bias로 쓰는 형태인데 128/512 Sequence Length에 대해 학습한 모델을 8K로 Extrapolation을 하는데 성공했네요. Positional Encoding에서 이런 정도의 개선이 있었던 적이 없는 것 같아서 계속 결과가 맞는지 확인하게 되네요.

<english>
Quite suprising result. The method is attention score which is applied convolution is used as an attention bias, and it is successed to extrapolate models trained with 128/512 sequence length into 8K. It makes me re-check the results as I haven't met these kind of improvements in positional encoding.
</english>

#long-context #attention #positional-encoding 