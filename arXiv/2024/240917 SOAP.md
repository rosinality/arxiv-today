https://arxiv.org/abs/2409.11321

*SOAP: Improving and Stabilizing Shampoo using Adam* (Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade)

> There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient approximation of Adam -- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: ShampoO with Adam in the Preconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.

Shampoo가 Shampoo에서 사용된 Preconditioner의 고유 기저에 대해 Adafactor를 적용한 것과 같다는 아이디어. 그렇다면 이 고유 기저에 대해 Adafactor 대신 Adam도 적용해볼 수 있겠죠. 흥미롭네요.

학습 속도가 증가한다는 것 - 같은 스텝에서 더 낮은 Loss를 달성할 수 있다는 것은 샘플 효율성이 높다는 것이고, 샘플 효율성은 곧 실질적으로 갖고 있는 데이터의 양을 늘려주는 효과로 생각할 수 있겠죠. 확보한 데이터의 양이 중요한 시점에서 그 의미가 더 크다고 할 수 있겠습니다.

#optimizer 