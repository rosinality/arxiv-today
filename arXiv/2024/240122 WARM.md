https://arxiv.org/abs/2401.12187

*WARM: On the Benefits of Weight Averaged Reward Models* (Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret)

> Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.

Reward Model 앙상블에 이어 Reward Model의 Weight Averaging이 따라나왔군요. Weight Averaging이 작동하면서 동시에 Diverse 하도록 데이터 순서와 하이퍼파라미터를 바꾸고 SFT 학습 과정의 서로 다른 시점에서 시작하는 방식을 시도했습니다. Validation 상황에서 앙상블보다 나은 성능을 보여주네요. 효율성까지 고려하면 반드시 참고해야할 방법으로 보입니다.

Weight Averaging도 꽤 오래된 방법인데 이런 곳에서 다시 등장하는군요.

#ensemble #reward-model #rlhf 