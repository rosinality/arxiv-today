https://arxiv.org/abs/2410.13857

*How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs* (Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang)

> Despite the remarkable success of Transformer-based Large Language Models (LLMs) across various domains, understanding and enhancing their mathematical capabilities remains a significant challenge. In this paper, we conduct a rigorous theoretical analysis of LLMs' mathematical abilities, with a specific focus on their arithmetic performances. We identify numerical precision as a key factor that influences their effectiveness in mathematical tasks. Our results show that Transformers operating with low numerical precision fail to address arithmetic tasks, such as iterated addition and integer multiplication, unless the model size grows super-polynomially with respect to the input length. In contrast, Transformers with standard numerical precision can efficiently handle these tasks with significantly smaller model sizes. We further support our theoretical findings through empirical experiments that explore the impact of varying numerical precision on arithmetic tasks, providing valuable insights for improving the mathematical reasoning capabilities of LLMs.

Low Precision 트랜스포머는 계산 문제에 대해서 표현력에 제약이 걸린다는 연구. 생각하지 못했던 문제인데 사실이라고 하면 흥미로운 문제네요.

<english>
This study suggests that low precision has representational limitation for arithmetic tasks. I haven't thought about this problem, and if it is real, then definitely it would be interesting problem.
</english>

#transformer 