https://arxiv.org/abs/2402.10524

*LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models* (Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, Lucas Dixon)

> Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.

두 LLM에 대한 비교 평가를 분석하기 위한 인터페이스. 카테고리나 판단 근거 같은 것들이 아무래도 주축이 되는군요. 최근 Preference 분류 성향에 대한 클러스터링 등으로 데이터에 대한 이해를 높일 수 있으리라는 생각을 해봤었는데 여러모로 생각해 볼만한 부분이 있지 않을까 싶습니다.

#evaluation #tool 