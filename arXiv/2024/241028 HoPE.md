https://arxiv.org/abs/2410.21216

*HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation* (Yuhan Chen, Ang Lv, Jian Luan, Bin Wang, Wei Liu)

> Many positional encodings (PEs) are designed to exhibit long-term decay, based on an entrenched and long-standing inductive opinion: tokens farther away from the current position carry less relevant information. We argue that long-term decay is outdated in the era of LLMs, as LLMs are now applied to tasks demanding precise retrieval of in-context information from arbitrary positions. Firstly, we present empirical analyses on various PEs, demonstrating that models inherently learn attention with only a local-decay pattern while forming a U-shape pattern globally, contradicting the principle of long-term decay. Furthermore, we conduct a detailed analysis of rotary position encoding (RoPE, a prevalent relative positional encoding in LLMs), and found that the U-shape attention is caused by some learned components, which are also the key factor limiting RoPE's expressiveness and extrapolation.Inspired by these insights, we propose High-frequency rotary Position Encoding (HoPE). HoPE replaces the specific components in RoPE with position-independent ones, retaining only high-frequency signals, which also breaks the principle of long-term decay in theory. HoPE achieves two major advantages: (1) Without constraints imposed by long-term decay, contradictory factors that limit spontaneous attention optimization and model extrapolation performance are removed. (2) Components representing positions and semantics are are optimized. These enhances model's context awareness and extrapolation, as validated by extensive experiments.

RoPE에서 저주파수 영역을 빼는 쪽이 더 낫다는 연구. 얼마 전 나온 연구와 통하는군요. (https://arxiv.org/abs/2410.06205) 중요한 부분은 Attention에 대한 Long term decay가 필요하지 않은 것 같다는 것일 것 같네요.

<english>
This study says that it is better to exclude low frequency region in RoPE. I think it is similar conclusion to previous study. (https://arxiv.org/abs/2410.06205) Important point is that maybe we don't need to employ long term decay on attention.
</english>

#positional-encoding #long-context

# Links

[[241008 Round and Round We Go! What makes Rotary Positional Encodings useful.md]]