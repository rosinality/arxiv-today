https://arxiv.org/abs/2401.14112

*FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design* (Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song)

> Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline. The source code will be publicly available soon.

8 bit보다는 빨랐으면 좋겠는데 4 bit에서 성능을 유지하는 게 쉽지 않다. 그래서 선택한 6 bit Quantization에 대한 최적화된 커널이네요.

학습에서나 추론에서나 더 낮은 Precision을 통한 더 빠른 연산을 생각하지 않을 수 없는 것 같네요. 6 bit Precision은 재미있는 선택인 것 같습니다. 관련해서 Microscaling (https://arxiv.org/abs/2310.10537) 같은 형태도 흥미로운 방향일 듯 싶네요.

#quantization #efficiency

# Links

[[231016 Microscaling Data Formats for Deep Learning.md]]