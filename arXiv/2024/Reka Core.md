https://publications.reka.ai/reka-core-tech-report.pdf

Reka에서 Reka Core를 공개하면서 Reka Flash와 Edge에 대한 정보도 공개했습니다. 7B/21B 모델이고 4.5T/5T 토큰에 대해 학습했군요. 카테고리가 배타적이지는 않은데 웹 크롤 데이터는 25% 수준이고 STEM 관련이 30%, 코드 관련 데이터가 25% 정도군요. 15%는 Multilingual입니다.

Multimodal 모델이고 Encoder-Decoder 구조네요. UL2도 들어간 듯 하네요. PaLM 아키텍처이고 높은 LR로 학습, 사용한 최대 클러스터는 A100 2.5K H100 2.5K인 듯 합니다.

벤치마크에서 Reka Core는 실제로 GPT-4 수준의 스코어네요. Reka Edge는 Mistral 7B와 Gemma 7B를 상회하는 수준의 결과가 나타났습니다. 연산력이라는 기본 조건이 갖춰지고 나면 모델을 만드는 방법에 대한 지식이 굉장한 차이를 만들어 낸다는 것이 다시 한 번 증명되는 듯 하네요.

#llm 


https://publications.reka.ai/reka-core-tech-report.pdf

Reka 테크니컬 리포트에서 찾아낼 수 있는 정보들을 정리. 일단 최고 성능 모델인 Reka Core는 Claude 3 Opus와 Sonnet 중간 정도에 있는 성능이다. ELO도 대략 그 사이에 있다. Mistral Large 보다 좀 더 고성능이라고 할 수 있을 듯. 그러나 이쪽은 이미지/비디오/오디오에 대한 Multimodal 모델이다.

학습 데이터의 카테고리를 보면 다음과 같다. 배타적인 분류가 아니기에 중복이 있을 수 있다. 5T 토큰을 기준으로 보면,

25% 코드 = 1.25T
30% STEM = 1.5T
25% 웹 크롤 = 1.25T
10% 수학 = 0.5T
15% Multilingual = 0.75T

Multi Epoch은 아니라고 한다. (https://x.com/YiTayML/status/1779903430053761444) 웹 크롤의 비중이 상당히 작다는 것이 특징적이다. 물론 1.25T도 총량으로는 적지 않다. 다만 그 이상의 STEM, 수학 데이터를 어떻게 확보했는지는 의문이 있다. DeepSeek Math에서 Common Crawl의 수학 관련 데이터를 탈탈 털어서 확보한 데이터의 규모가 120B 수준이었다. (https://arxiv.org/abs/2402.03300) 아마도 책이나 논문 데이터를 굉장히 많이 확보한 것이 아닐까 싶다.

Gemma 7B가 6T, Mistral 7B도 6T 정도라고 하는데 Reka Edge 7B가 4.5T 정도로 이 두 모델을 상회하는 스코어를 달성했다는 것은 데이터 측면의 강점을 다시 보여주는 것이 아닐까 싶다.

모델 구조는 PaLM과 유사한데 인코더-디코더이다. MoE는 아니다. 텍스트에는 인코더를 사용하지 않았다 같은 세팅은 아닐 듯 하고 센티널 토큰이 들어가 있다는 것을 보면 T5, UL2 계통의 모델일 가능성이 높다.

확보한 GPU는 H100 2.5K A100 2.5K 정도. 주로 H100으로 학습시켰다고 하고 추측컨데 3개월 정도 학습했다고 하면 70B 규모의 모델을 12T 정도 학습시킬 수 있지 않을까 싶다. 물론 인코더-디코더이므로 성능 패턴이 많이 다르긴 할 것이다.

비교적 높은 Learning Rate를 사용한 듯 하다. 학습이 가능한 한 가장 높은 Learning Rate를 사용하는 것이 좋다는 직관은 여전히 통하는 규칙인 듯 하다.

Reka가 $60M 투자를 받아 시작했다고 알고 있는데 이 정도의 비용과 20명 규모의 팀으로 이런 모델을 만들 수 있다는 중요한 사례라고 할 수 있을 것이다. 반대로 모델을 만드는 방법을 모른다면 더 많은 비용을 사용하더라도 이런 수준의 모델을 만들기 어렵다는 것을 알려주는 사례이기도 하다.

그래서 대체 그 방법이 무엇인가? 직접 이것저것 해보더라도 알기 쉽지 않을 문제일 듯 싶다. 다만 지금까지 주어진 힌트를 기반으로 7B 수준에서부터 비슷한 스코어를 뽑는 것을 목표로 해야 하지 않을까 싶다. 물론 7B 모델을 4.5T 학습시키는 것도 만만치 않은 일이긴 하다.