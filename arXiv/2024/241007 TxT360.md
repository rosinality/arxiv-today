https://huggingface.co/spaces/LLM360/TxT360

*TxT360: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend* (Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Yue Peng, Eric P. Xing, Zhengzhong Liu)

> 

LLM 프리트레이닝을 위한 코퍼스인데 Common Crawl 뿐만 아니라 StackExchange 같은 데이터 소스도 전처리하고 Global Deduplication을 했네요.

Common Crawl의 경우에는 Trafilatura + 휴리스틱 필터링 기반입니다. 모델 기반 필터링은 사용하지 않았네요. 특이하게도 Duplication 횟수에 따라 Upsampling을 하는 방법을 사용했습니다. Duplicate의 수가 문서의 퀄리티에 대한 간접적인 지표가 될 수 있다는 직관입니다.

<english>
Corpus for pretraining LLMs. Not only Common Crawl datasources like StackExchange are preprocessed and global deduplication was done.

For Common Crawl Trafilatura and heuristic filtering was employed. Model based filtering was not used. Interestingly, upsampling was done relative to the number of duplication. The idea is that number of duplication can be a indirect metrict for quality of documents.
</english>

#corpus 