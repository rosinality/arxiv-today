https://arxiv.org/abs/2410.13859

*$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models* (Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji)

> Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue, we propose an innovative MoD adaptation strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer. Based on ARank, we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely shared vision-language router and masked routing learning. With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets. Experimental results not only validate the significant efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs. For example, with a minor performance drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively.

기존 Multimodal 모델에 Mixture of Depths를 (https://arxiv.org/abs/2404.02258) 적용하는 방법. Mixture of Depths를 적용할 레이어는 Attention 행렬의 랭크를 통해 찾아냅니다. 랭크가 낮다면 토큰에 중복이 많다는 것이죠. Multimodal 모델에 대해서는 다들 정보량이 낮은 토큰이 많다고 생각해서인지 Mixture of Depths를 적용한 사례가 종종 나오네요. (https://arxiv.org/abs/2408.16730)

<english>
Applying mixture of depths (https://arxiv.org/abs/2404.02258) to pre-existing models. This study find out the layers that mixture of depths would be applied by using the rank of the attention matrix. If attention matrix is low rank then it suggest there are redundancy on tokens. As many people thoughts for multimodal models there are many tokens with low information content, there are tries to apply mixture of depths to it. (https://arxiv.org/abs/2408.16730)
</english>

#multimodal #moe #efficiency

# Links

[[240829 VideoLLM-MoD.md]]
[[240402 Mixture-of-Depths.md]]