https://arxiv.org/abs/2401.15077

*EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty* (Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang)

> Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface's implementations.

Speculative Sampling에 대한 개선. 토큰을 예측하는 것보다 임베딩을 예측하는 것이 쉽다고 하고, 따라서 다음 스텝의 임베딩을 예측하고 LM Head로 토큰을 생성합니다.

그런데 다음 스텝의 임베딩은 어떤 토큰이 샘플링 되어 Forward 되는가에 따라 달려있죠. 그래서 샘플링한 토큰에 대한 임베딩도 같이 입력으로 받아 예측합니다.

#efficiency 

Improvement to Speculative Sampling. This study suggest that predicting embedding is easier than predicting tokens. Therefore this method predicts embeddings at the next step and generate tokens with frozen LM head.

But embedding of next step depends on which token is sampled and forwarded. Therefore it also uses embedding of the sample tokens to predict the subsequent embedding.