https://arxiv.org/abs/2407.17465

*u-μP: The Unit-Scaled Maximal Update Parametrization* (Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr)

> The Maximal Update Parametrization (μP) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-μP, which improves upon μP by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: μP ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-μP models reaching a lower loss than comparable μP models and working out-of-the-box in FP8.

Graphcore의 μP 개선. Unit Scaling이라는 (https://arxiv.org/abs/2303.11257) Low Precision을 위한 테크닉을 결합하면서 탐색할 하이퍼파라미터를 줄이고 하이퍼파라미터가 더 잘 Transfer 되도록 개선했다고 합니다. 그런데 Graphcore지만 실험은 A100/H100으로 했네요.

#hyperparameter

# Links

