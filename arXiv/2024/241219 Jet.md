https://arxiv.org/abs/2412.15129

*Jet: A Modern Transformer-Based Normalizing Flow* (Alexander Kolesnikov, André Susano Pinto, Michael Tschannen)

> In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models.

구글도 Normalizing Flow 실험을 해봤군요. (https://arxiv.org/abs/2412.06329) 다른 어떤 요소 없이 Affine Coupling만으로도 충분하더라는 결과. (단 채널이 아닌 공간에 대해서도 분할하는 방법을 섞었네요.)

<english>
Google also have done experiments on normalizing flow. (https://arxiv.org/abs/2412.06329) The result say that affine coupling without any other components. (But mixed not only along the channels, but also along the spatial axes.>
</english>

#flow

# Links

[[241209 Normalizing Flows are Capable Generative Models.md]]