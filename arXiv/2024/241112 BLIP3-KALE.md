https://arxiv.org/abs/2411.07461

*BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions* (Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, Jae Sung Park, Etash Guha, Silvio Savarese, Ludwig Schmidt, Yejin Choi, Caiming Xiong, Ran Xu)

> We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale

CogVLM으로 생성한 캡션과 Alt text를 결합해 Recaptioning. 다만 CogVLM에 대해 차이가 아주 크다는 느낌은 아니긴 하네요.

<english>
Recaptioning by combining generated captions using CogVLM and alt text. But it is like that difference of performance between this method and captions generated by CogVLM is not very large.
</english>

#synthetic-data #image-text #captioning 