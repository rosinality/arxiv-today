https://arxiv.org/abs/2401.12522

*BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models* (Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong Xiao)

> Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments confirm our method surpasses state-of-the-art acceleration techniques.

Speculative Decoding의 연장선상에서 한 번에 여러 토큰을 예측하는 시도들이 많이 나오는군요. Medusa가 헤드를 여러 개 만들었다면 (https://arxiv.org/abs/2401.10774) 여기에선 마스크 토큰 여러 개를 입력하고 P-Tuning을 하는 형태네요.

개인적으로는 이런 N개 토큰 예측은 프리트레이닝에도 들어갈 수 있지 않을까 싶습니다. 추론 속도 가속 외에도 추가적인 효과를 줄 수 있을 듯 하네요.

#efficiency

# Links

