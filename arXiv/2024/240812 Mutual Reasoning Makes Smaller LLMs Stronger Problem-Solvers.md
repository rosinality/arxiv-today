https://arxiv.org/abs/2408.06195

*Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers* (Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang)

> This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be available at https://github.com/zhentingqi/rStar.

LLM + MCTS. 각 단계에서 LLM이 취할 수 있는 액션을 다양화하고, Self Evaluation은 한계가 있다고 판단해서 다시 정답 여부를 Reward로 사용했군요. 대신 정답 선택을 다른 LLM을 사용해 Trajectory의 초기 부분을 주고 나머지를 생성시켰을 때 같은 정답으로 이어지는 점검하는 방법을 사용했습니다.

#search 