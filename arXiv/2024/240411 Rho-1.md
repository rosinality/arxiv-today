https://arxiv.org/abs/2404.07965

*Rho-1: Not All Tokens Are What You Need* (Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen)

> Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that "Not all tokens in a corpus are equally important for language model training". Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.

일단 학습 과정에서 Loss가 꾸준히 낮은 경우, Loss가 높았다가 낮아지는 경우, 오히려 높아지는 경우, 그리고 계속 높은 경우가 있다는 구분에서 시작하네요. 후자 두 가지는 노이즈인 경우가 많다고 합니다.이 발견을 어떻게 활용할 것인가? 레퍼런스 모델을 높은 퀄리티의 데이터셋에 학습시킨 다음 현재 학습 중인 모델과의 Loss 차이가 큰 경우만 골라서 학습시키는 방법을 사용했습니다. 수학 도메인에서 실험했는데 효과가 굉장하네요.

Hard Example Mining이나 Focal Loss가 생각나는군요. 다만 단순히 Loss가 높은 것을 고르는 것이 아니라 노이즈로 인해 높은 경우를 걸러내는 효과가 중요한 것이 아닐까 싶습니다.

#pretraining #llm 