https://blog.hippoml.com/8bit-hippoattention-up-to-3x-faster-compared-to-flashattentionv2-8f9def90b482

FP8 Attention들이 나오기 시작하는군요. (https://blog.fireworks.ai/fireattention-serving-open-source-models-4x-faster-than-vllm-by-quantizing-with-no-tradeoffs-a29a85ad28d0) Flash Attention v2의 1.5 ~ 3배 가량의 수치를 보여주고 있네요.

저도 FP8을 써보고 싶네요. 4090이라도 구해와야할지.