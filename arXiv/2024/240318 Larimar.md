https://arxiv.org/abs/2403.11901

*Larimar: Large Language Models with Episodic Memory Control* (Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Navrátil, Soham Dan, Pin-Yu Chen)

> Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.

와 Kanerva Machine! (https://arxiv.org/abs/1804.01756, https://openreview.net/forum?id=Harn4_EZBw) 정말 오랜만에 보네요. 읽고 쓰는 것이 가능한 메모리 메커니즘을 추가해 새로운 지식을 저장하고 편집할 수 있는 능력을 부여하겠다는 아이디어입니다.

메모리 메커니즘을 추가하는 것은 오래도록 계속되어온 꿈이죠. 단순하고 강력한 메모리 메커니즘을 추가할 수 있다면 실제로 꽤 의미가 있지 않을까 싶습니다. 아직 사례는 없지만요.

#memory

# Links

