https://arxiv.org/abs/2401.16788

*Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate* (Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu)

> Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \url{https://github.com/GAIR-NLP/scaleeval}.

모델을 사용한 평가가 사람의 평가와 잘 정렬되어 있는지를 판단하기 위한 방법. 사람의 평가 데이터를 모델로 대체하기 위한 방법입니다. 여기서는 여러 모델에게 평가를 하게 한 다음, 각 모델이 서로의 평가 결과를 보고 자신의 평가를 업데이트해서 합의에 이르게 합니다. 합의하지 못한 경우에는 사람이 직접 어노테이션을 하고요.

모델을 쓰는 방법이기에 사람이 이 방법이 잘 정렬되어 있는지에 대한 메타-메타 평가를 진행했습니다. 반대로 이 방법 자체를 더 나은 평가 방법으로서 사용할 수 있지 않을까 하는 생각도 드네요.

#evaluation 

This paper proposes the method that judge evaluations done by models are well-aligned with humans. Goal of this method is replacing human evaluation data with models. In this method multiple models evaluates response pairs, and then answer of the other models given to each other. Then model updates their judgments, and reach consensus. If consensus is not accomplished then human annotates the case directly.

As this method employs model for evaluations, whether this method is well aligned with humans are evaluated by humans, which is meta-meta evaluations. Conversely I think this method could be used for better evaluation methods for automated evaluations.