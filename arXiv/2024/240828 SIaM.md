https://arxiv.org/abs/2408.15565

*SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models* (Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, Dong Yu)

> There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in-domain data augmentation, equipping LLMs with considerable capabilities for code-aided mathematical reasoning. However, continually training these models on augmented data derived from a few datasets such as GSM8K may impair their generalization abilities and restrict their effectiveness to a narrow range of question types. Conversely, the potential of improving such LLMs by leveraging large-scale, expert-written, diverse math question-answer pairs remains unexplored. To utilize these resources and tackle unique challenges such as code response assessment, we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation. We also explore different alignment algorithms with self-generated instruction/preference data to foster continuous improvement. Experiments across both in-domain (up to +5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate the effectiveness of the proposed paradigm.

코드 기반으로 수학 문제를 푸는 방법에 대한 탐색. 벤치마크 데이터를 기반으로 생성하는 형태의 시도가 많이 있었지만 언제까지나 벤치마크만 파고 있을 수는 없죠.

따라서 이를 웹 데이터 기반으로 해보려는 시도입니다. 웹 데이터에서 정답을 추출하기는 어려우니 Critic 모델을 사용해서 질문, 모델이 생성한 코드, 코드 실행 결과, 정답을 주고 분류하게 합니다.

Scalable한가? 이 질문이 방법을 평가하는데 있어 꽤 유용하다고 봅니다. 이는 모델에 대해서 뿐만 아니라 데이터 구축에 대해서도 마찬가지죠.

#code #synthetic-data 