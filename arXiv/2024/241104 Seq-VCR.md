https://arxiv.org/abs/2411.02344

*Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning* (Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal)

> Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model's intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging $5 \times 5$ integer multiplication task, our approach achieves $99.5\%$ exact match accuracy, outperforming models of the same size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting ($44\%$). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.

계산 과제를 트랜스포머가 잘 학습하지 못하는 이유가 Representation의 다양성이 감소하는 경향 때문이라는 주장. 이에 대해 Regularization을 걸어주는 방법으로 문제를 해소했습니다. Representation Collapse가 왜 일어나는 걸까요? 흥미로운 문제 같습니다.

<english>
Suggestion that the reason why transformers cannot solve arithmetic tasks well resides on decreasing diversity of representations. They reduced this problem by applying regularization on the diversity of representation. What would be the reason representation collapse happens? It seems like an interesting problem.
</english>

#regularization #reasoning #transformer 