https://arxiv.org/abs/2401.02669

Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache (Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong Xiao, Qi Xu, Xiafei Qiu, Shen Li, Zhigang Ji, Yong Li, Wei Lin)

Long Context LLM 시대의 서빙 프레임워크. Attention과 나머지 레이어가 서로 다른 Model Parallel 세팅을 쓸 수 있게 만들고 KV 캐시를 여러 CPU와 GPU에 분산해서 관리할 수 있게 만들었습니다. 서빙 관련해서 할 수 있는 일과 해야 하는 것들이 정 많다 싶습니다.

#long_context #efficiency