https://arxiv.org/abs/2407.14622

*BOND: Aligning LLMs with Best-of-N Distillation* (Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, Olivier Bachem)

> Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models. Yet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates. In this paper, we propose Best-of-N Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant computational overhead at inference time. Specifically, BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution. We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving anchor for efficiency. We demonstrate the effectiveness of our approach and several design choices through experiments on abstractive summarization and Gemma models. Aligning Gemma policies with BOND outperforms other RLHF algorithms by improving results on several benchmarks.

Gemma에서 언급된 새로 개발한 RL 알고리즘의 정체가 밝혀졌군요. BoN 분포를 모사하도록 학습하는 방법이네요. 이 아이디어는 얼마 전에도 나왔었죠. (https://arxiv.org/abs/2407.06057) 이쪽의 차이는 Jeffreys Divergence를 사용해 Forward KL과 Reverse KL의 선형 결합을 최적화 한다는 것이네요. 동시에 비슷한 방법이 나왔다는 점에서 왠지 괜찮지 않을까 싶은 생각이 드네요.

#alignment #rlhf

# Links

