https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf

*Gemma 2: Improving Open Language Models at a Practical Size* (Gemma Team, Google DeepMind)

> In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. The 9 billion and 27 billion parameter models are available today, with a 2 billion parameter model to be released shortly. In this new version, we provide several technical modifications to our architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community.

Gemma 2가 나왔습니다. 2.6B (2T), 9B (8T), 27B (13T) 모델. Gemma의 굉장히 큰 FFN은 줄어든 대신 깊은 모델이 됐네요. 추가로 4096 Local Attention과 8192 Global Attention의 조합, Grok-1에서 등장한 tanh를 사용한 Attention Logit 클리핑, Group 2 GQA 등을 도입했습니다. Pre Norm과 Post Norm을 썼다는 언급이 다시 등장했는데 Gemma 2에서는 정말로 Post Norm을 쓴 것 같네요.

벤치마크 스코어는 잘 나왔습니다. Gemma 2 9B는 Llama 3 8B를 상회하고 Gemma 2 27B는 Llama 3 70B에 근접하는 수준의 스코어. Chatbot Arena에서도 Sonnet 3와 Llama 3 70B를 상회하는 지점에 있습니다. Chatbot Arena의 프롬프트로 학습을 하긴 했습니다만.

그러나 이런 부분들을 차치하면 가장 중요한 것은 Gemma 1.5 Flash에서도 언급되었던 Knowledge Distillation이 적극적으로 사용됐다는 것이겠네요. 여기서 좀 더 특이한 것은 Distillation을 대규모의 토큰에 대해 학습하면서 확보된 양의 토큰을 넘어서는 수준의 학습을 시뮬레이션하는 방법이라고 설명하고 있다는 것입니다. 이것의 의미가 무엇인지 생각해볼 필요가 있을 듯 하네요. Distillation과 관련해서 Gemini 1.5 Flash에 언급된 논문 중 특징적인 세 논문이 있는데 (https://arxiv.org/abs/1804.03235, https://arxiv.org/abs/2106.05237, https://arxiv.org/abs/2306.13649) 이쪽을 참조해보는 것이 의미가 있을 듯 합니다.

SFT 과정에서도 프롬프트에 대해 Teacher가 생성한 응답을 가지고 학습시키고, 동시에 Distillation을 이쪽에 대해서도 적용했다고.

전반적으로 하이퍼파라미터를 계속 조정하고 있는 것을 보면 의미 있는 하이퍼파라미터라는 느낌이 있고, Local Attention과 Global Attention을 조합하는 것 등은 최근 Character.AI에서도 소개한 것을 보면 (https://research.character.ai/optimizing-inference/) 중요한 트릭인 것 같기도 합니다. Knowledge Distillation도 유의미한 것 같은데 이쪽은 일단 큰 모델을 제대로 학습하는 것이 선행 과제이긴 하죠.

AI Studio에 올라가 있는데 한국어도 꽤 합니다. 한 번 테스트해보시는 것도.

#llm #distillation 