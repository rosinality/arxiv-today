https://arxiv.org/abs/2408.11743

*MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large Language Models* (Elias Frantar, Roberto L. Castro, Jiale Chen, Torsten Hoefler, Dan Alistarh)

> As inference on Large Language Models (LLMs) emerges as an important workload in machine learning applications, weight quantization has become a standard technique for efficient GPU deployment. Quantization not only reduces model size, but has also been shown to yield substantial speedups for single-user inference, due to reduced memory movement, with low accuracy impact. Yet, it remains open whether speedups are achievable also in \emph{batched} settings with multiple parallel clients, which are highly relevant for practical serving. It is unclear whether GPU kernels can be designed to remain practically memory-bound, while supporting the substantially increased compute requirements of batched workloads. This paper resolves this question positively by describing the design of Mixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely, given a model whose weights are compressed via quantization to, e.g., 4 bits per element, MARLIN shows that batchsizes up to 16-32 can be supported with close to maximum ($4\times$) quantization speedup, and larger batchsizes up to 64-128 with gradually decreasing, but still significant, acceleration. MARLIN accomplishes this via a combination of techniques, such as asynchronous memory access, complex task scheduling and pipelining, and bespoke quantization support. Our experiments show that MARLIN's near-optimal performance on individual LLM layers across different scenarios can also lead to end-to-end LLM inference speedups (of up to $2.8\times$) when integrated with the popular vLLM serving engine. Finally, MARLIN is extensible to further compression techniques, like NVIDIA 2:4 sparsity, leading to additional speedups.

W4A16 커널이군요. Llama 3 405B의 FP8 적용으로 인한 열화가 충격적이었는데 W4A16이 오히려 나을 수도 있지 않을까 싶네요. 별개로 Int8이나 FP8 Native Training이 답이 아닐까 싶기도 하고...그렇습니다. FP8 학습에 대한 경험담들도 조금씩 나오네요. (https://x.com/xariusrke/status/1826669126955278401)

#quantization 