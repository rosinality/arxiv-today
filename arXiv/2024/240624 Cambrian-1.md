https://arxiv.org/abs/2406.16860

*Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs* (Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, Saining Xie)

> We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.

Multimodal 모델의 컴포넌트, 이미지 인코더, 커넥터 등등에 대한 비교 분석. 5 페이지에 등장한 아주 근본 넘치는 이미지가 반갑네요. 그나저나 르쿤 선생님도 뉴욕대에서는 LLM 논문 저자로 들어가시긴 하네요.

개인적으로는 From Scratch 혹은 그에 가까운 멀티모달 연구 결과도 재미있는 방향이 아닐까 싶습니다. 다만 연산요구량에 너무 현격한 차이가 있으니 쉽지는 않겠네요.

#vision-language #multimodal 