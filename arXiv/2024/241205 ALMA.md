https://arxiv.org/abs/2412.04305

*ALMA: Alignment with Minimal Annotation* (Michihiro Yasunaga, Leonid Shamis, Chunting Zhou, Andrew Cohen, Jason Weston, Luke Zettlemoyer, Marjan Ghazvininejad)

> Recent approaches to large language model (LLM) alignment typically require millions of human annotations or rely on external aligned models for synthetic data generation. This paper introduces ALMA: Alignment with Minimal Annotation, demonstrating that effective alignment can be achieved using only 9,000 labeled examples -- less than 1% of conventional approaches. ALMA generates large amounts of high-quality synthetic alignment data through new techniques: diverse prompt synthesis via few-shot learning, diverse response generation with multiple model checkpoints, and judge (reward model) enhancement through score aggregation and self-distillation. Using only a pretrained Llama3 base model, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves performance close to Llama3-Instruct across diverse alignment benchmarks (e.g., 0.1% difference on AlpacaEval 2.0 score). These results are achieved with a multi-round, self-bootstrapped data synthesis and training recipe that continues to improve for 10 rounds, surpassing the typical 3-round ceiling of previous methods. These results suggest that base models already possess sufficient knowledge for effective alignment, and that synthetic data generation methods can expose it.

소규모 시드 데이터로 모델 정렬. 프롬프트를 증폭하고 최대한 다양한 응답을 생성하는 것, 그리고 LLM Judge를 사용하면서 정수 대신 실수 스코어를 사용한 것 등이 주요 요소군요.

<english>
Aligning the model with small scale seed data. The main point is amplifying the prompt and generating diverse responses, and using LLM judges with real score instead of integer score.
</english>

#alignment #reward-model 