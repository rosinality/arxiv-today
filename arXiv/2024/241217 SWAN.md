https://arxiv.org/abs/2412.13148

*SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction* (Chao Ma, Wenbo Gong, Meyer Scetbon, Edward Meeds)

> Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the success of large language models. However, they maintain additional moving average states throughout training, which results in memory requirements several times greater than the model. This overhead imposes constraints on scalability and computational efficiency. On the other hand, while stochastic gradient descent (SGD) is optimal in terms of memory efficiency, their capability in LLM training is limited (Zhao et al., 2024b). To address this dilemma, we show that pre-processing SGD is sufficient to reach Adam-level performance on LLMs. Specifically, we propose to preprocess the instantaneous stochastic gradients with two simple operators: $\mathtt{GradNorm}$ and $\mathtt{GradWhitening}$. $\mathtt{GradNorm}$ stabilizes gradient distributions, and $\mathtt{GradWhitening}$ counteracts the local curvature of the loss landscape, respectively. This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any accumulative state variables. Empirically, SWAN has the same memory footprint as SGD, achieving $\approx 50\%$ reduction on total end-to-end memory compared to Adam. In language modeling tasks, SWAN demonstrates the same or even a substantial improvement over Adam. Specifically, when pre-training the LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation perplexity in less than half tokens seen.

Optimizer State들을 모두 제거하고 대신 그래디언트에 대한 정규화와 화이트닝으로 대체한다는 아이디어. 화이트닝 연산의 비용이 문제일 텐데 여기서는 그럭저럭 괜찮다고 하는군요.

Optimizer State의 크기 자체가 문제가 되면서 오히려 Optimizer 연구가 더 많이 나오는 듯한 느낌도 드네요.

<english>
The idea that remove all optimizer states and replace it with normalization and whitening of gradients. The cost of computation of whitening would be problem, but the paper says that it is manageable.

As the size of optimizer itself became a problem it seems optimizer researches appear more frequently.
</english>

#optimizer 