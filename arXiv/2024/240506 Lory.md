https://arxiv.org/abs/2405.03133

*Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training* (Zexuan Zhong, Mengzhou Xia, Danqi Chen, Mike Lewis)

> Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.

Expert FFN의 파라미터를 가중 평균한 FFN으로 Forward해서 미분 가능한 Mixture of Experts 모델을 만드는 전략. (https://arxiv.org/abs/2306.03745) 그러나 가중 평균 자체가 비싸기 때문에 시퀀스를 세그먼트로 자른 다음 이전 세그먼트에 대해 계산한 가중치로 다음 세그먼트에 사용할 FFN을 생성하는 전략 + 서로 비슷한 텍스트로 시퀀스를 구성해 학습을 쉽게 만드는 트릭을 사용했습니다. Super Token을 쓰는 전략도 생각나네요. (https://arxiv.org/abs/2311.10768)

#moe

# Links

[[231115 Memory Augmented Language Models through Mixture of Word Experts.md]]