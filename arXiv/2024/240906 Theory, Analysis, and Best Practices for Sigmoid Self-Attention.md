https://arxiv.org/abs/2409.04431

*Theory, Analysis, and Best Practices for Sigmoid Self-Attention* (Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russ Webb)

> Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.

Attention에서 Softmax 대신 Sigmoid를 사용한 트랜스포머. 왜 Sigmoid를 사용해야 하는가라고 하면 FlashAttention 같은 최적화를 했을 때 좀 더 빠르고 Lipschitz constant의 상한이 Softmax Attention보다 작다는 이야기를 합니다. 다만 그보다는 Sigmoid Attention으로 Softmax Attention의 성능을 낼 수 있는 세팅을 찾았다가 요점인 것 같긴 하네요.

#transformer 