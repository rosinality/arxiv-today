https://arxiv.org/abs/2402.01345

*Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models* (Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike Zheng Shou)

> Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '$\textbackslash n\textbackslash n$'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '$\textbackslash n\textbackslash n$' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of `\textbackslash n'.

이건 좀 신박한 아이디어네요. Vision Language 모델의 생성 과정에서 문단 바꿈(개행 두 번)이 발생하면 이전 문단과 내용이 바뀌면서 할루시네이션이 발생할 수 있다는 아이디어입니다. 따라서 문단 바꿈이 들어가기 이전과 이후에 할루시네이션의 정도가 다르고, 일부러 문단 바꿈을 넣거나 억제해서 할루시네이션을 유도하거나 감소시킬 수 있다는 결과입니다.

데이터셋 이슈일까요? 일단 GPT-4에서는 이런 현상이 나타나지 않는다고 하네요. 다만 이는 할루시네이션을 억제하기 위한 튜닝의 영향도 있겠죠.

#hallucination #vision-language 

Neat idea! If paragraph break occurs, (that is, 2 consequtive new lines) during generation of vision-language models, semantic change occurs, and it could induces halluciations. So halluncination rate before or after paragraph break is different, and if we deliberately adds or suppresses paragraph breaks, then we can induces or reduces hallucinations.

Is this could be dataset issue? Author says that GPT-4 does not have this problem. But that could be the effect of tuning that was done it to reduce hallucinations.