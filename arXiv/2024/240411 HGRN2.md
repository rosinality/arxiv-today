https://arxiv.org/abs/2404.07904

*HGRN2: Gated Linear RNNs with State Expansion* (Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong)

> Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.

Mamba 같은 State Space Model이나 RWKV-5/6와 같이 Matrix valued State를 RNN에 결합하기 위한 시도. 결과적으로는 Gated Linear Attention과 거의 같은 형태가 됩니다. (https://arxiv.org/abs/2312.06635)

여전히 Attention을 필요할 것 같지만 State Space Model과 Attention의 하이브리드의 트랜스포머에 대비한 한계가 있는가? 하면 아직까지 그런 문제가 표면적으로는 드러나지 않는 것 같네요.

#state-space-model

# Links

