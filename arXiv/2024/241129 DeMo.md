https://arxiv.org/abs/2411.19870

*DeMo: Decoupled Momentum Optimization* (Bowen Peng, Jeffrey Quesnelle, Diederik P. Kingma)

> Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects. Drawing from the signal processing principles of frequency decomposition and energy compaction, we demonstrate that synchronizing full optimizer states and model parameters during training is unnecessary. By decoupling momentum updates and allowing controlled divergence in optimizer states across accelerators, we achieve improved convergence compared to state-of-the-art optimizers. We introduce {\textbf{De}}coupled {\textbf{Mo}}mentum (DeMo), a fused optimizer and data parallel algorithm that reduces inter-accelerator communication requirements by several orders of magnitude. This enables training of large neural networks even with limited network bandwidth and heterogeneous hardware. Our method is topology-agnostic and architecture-independent and supports scalable clock-synchronous distributed training with negligible compute and memory overhead. Empirical results show that models trained with DeMo match or exceed the performance of equivalent models trained with AdamW, while eliminating the need for high-speed interconnects when pre-training large scale foundation models. An open source reference PyTorch implementation is published on GitHub at https://github.com/bloc97/DeMo

모델 학습의 통신량을 줄이기 위한 방법. 모멘텀에서 빠르게 변화하는 부분과 느리게 변화하는 부분을 분리한 다음 빠르게 변화하는 부분만 동기화하는 방법이군요.

최근에 탈중앙화된 학습 방법으로 학습하고 있었던 모델 하나가 학습을 완료했습니다. (https://x.com/PrimeIntellect/status/1859923050092994738) 데이터 센터 하나에서 학습을 할 수 없는 규모가 된다고 하면 이런 방법들에 대한 주목도가 높아질 수밖에 없겠죠.

<english>
Reducing network communication needed in model training. Separating fast and slow moving component in the momentum and synchronize only fast moving one.

Recently training of a model is completed which is trained with decentralized training methods. (https://x.com/PrimeIntellect/status/1859923050092994738) If scale of training become larger than the size that is possible in one data center then these kind of method would got a traction.
</english>

#efficient-training 