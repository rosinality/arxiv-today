https://arxiv.org/abs/2410.02089

*RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning* (Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, Gabriel Synnaeve)

> Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.

Execution Feedback을 사용해 모델을 학습시키는 방법인데 여기의 핵심은 추론 시점에서 Execution Feedback을 사용해 결과를 개선할 수 있도록 학습시키는 것에 있네요. 공개/비공개 테스트를 나눈 다음 공개 테스트의 결과를 입력으로 사용해 개선하도록 루프를 돌고, 완료 시점에서 비공개 테스트의 결과를 합쳐 PPO를 하는 방식입니다.

<english>
A method of using execution feedback to train the model. Important point is that this work tries to train models to be able to improve results using execution feedback at inference time. Split public/private test, and do a loop that improve results by using the result of public test as an input, and at the finish step combine the result from private test sets and do the PPO.
</english>

#feedback #reasoning 