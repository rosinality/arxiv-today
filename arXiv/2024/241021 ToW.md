https://arxiv.org/abs/2410.16235

*ToW: Thoughts of Words Improve Reasoning in Large Language Models* (Zhikun Xu, Ming Shen, Jacob Dineen, Zhaonan Li, Xiao Ye, Shijie Lu, Aswin RRV, Chitta Baral, Ben Zhou)

> We introduce thoughts of words (ToW), a novel training-time data-augmentation method for next-word prediction. ToW views next-word prediction as a core reasoning task and injects fine-grained thoughts explaining what the next word should be and how it is related to the previous contexts in pre-training texts. Our formulation addresses two fundamental drawbacks of existing next-word prediction learning schemes: they induce factual hallucination and are inefficient for models to learn the implicit reasoning processes in raw texts. While there are many ways to acquire such thoughts of words, we explore the first step of acquiring ToW annotations through distilling from larger models. After continual pre-training with only 70K ToW annotations, we effectively improve models' reasoning performances by 7% to 9% on average and reduce model hallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks and applications, introducing no additional biases on labels or semantics.

다음 단어 예측을 위한 추론 과정을 생성하기 위한 방법. 이전 문맥을 주고 다음 단어가 무엇일지 추론한 다음 단어를 예측하고, 예측 결과와 텍스트를 비교해 필터링하는 방법입니다. 이런 행간 생성에는 계속 관심이 가네요. GPT-4를 써서 생성한 것이라 좀 아쉽긴 합니다만.

<english>
Synthesizing reasoning process for predicting next words. Give previous context to the model, and let model to reason what would be the next word and next word itself, and filter the data by comparing with actual text and predicted words. I always interested on generating the text between the lines. It would be better if they have not used GPT-4 to generated the text.
</english>

#reasoning #synthetic-data 