https://arxiv.org/abs/2401.08967

*ReFT: Reasoning with Reinforced Fine-Tuning* (Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li)

> One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.

CoT를 하게 한 다음 최종 답이 맞는지로 Reward를 주고 PPO. (너무나 당연한 소리인 것 같긴 하지만) 답을 구할 수 있으면 할 수 있는 것이 많다는 사례가 하나 추가되는군요.

Multiple Choice 문제에서는 답의 가짓수가 적으니 CoT는 틀렸는데 답은 맞을 경우가 있고 그 경우에 문제가 생기는 사례를 발견했네요. 재미있습니다.

#rl 