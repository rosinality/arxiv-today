https://github.com/kyutai-labs/moshi

*Moshi: a speech-text foundation model for real-time dialogue* (Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, Neil Zeghidour)

> We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning— such as emotion or non-speech sounds— is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this “Inner Monologue” method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.

Kyutai가 (https://kyutai.org/) 자신들의 Speech-Text 모델을 공개하면서 테크니컬 리포트도 같이 공개했네요.

언어 모델인 Helium과 오디오 코덱 모델인 Mimi로 구성되어 있습니다.

Helium은 Llama와 비슷한 세팅이군요. WET 기반으로 Line Dedup, fastText로 Duplicate vs Non-duplicate를 분류하는 방식으로 Fuzzy Dedup. 이쪽은 좀 특이하네요. 여기에 9개 카테고리에 대한 fastText 분류기로 라인 기반 퀄리티 필터링. 이쪽도 좀 특이하네요.

오디오 코덱 모델 Mimi는 RQ-VAE 기반 Autoregressive 모델이고 WavLM으로 Semantic한 정보를 Distill했습니다.

그 다음 Helium에 Depth Transformer를 붙여 RQ Transformer로 수정한 다음 학습시키는군요. 1단계에서는 단일 스트림으로 학습시키지만 2단계에서는 유저와 Moshi의 음성 스트림을 고려해 멀티 스트림으로 학습시킵니다. 그리고 텍스트와 오디오를 정렬한 다음 텍스트 토큰이 오디오 토큰의 Prefix로 기능하도록 만들었네요.

재미있네요. 다만 더 단순하게 할 수 있는 방법을 생각해보는 것도 재미있을 것 같습니다.

#speech-text