https://arxiv.org/abs/2401.02415

LLaMA Pro: Progressive LLaMA with Block Expansion (Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan)

Layer Stacking. SOLAR (https://arxiv.org/abs/2312.15166) 같은 접근과는 달리 Function Preserving Expansion을 사용했고 하부 레이어는 얼렸군요. 이 디자인 덕에 코드와 수학 위주로 데이터를 밀어넣었는데 자연어 과제들에서도 성능이 향상되었다는 것을 중점적으로 소개하고 있긴 합니다. (수학 코퍼스 덕이지 않을까 하는 생각도 들긴 합니다만.) MoE와의 비교 등도 눈에 띄네요.

#efficient_training

# Links

[[231229 Layer Stacking.md]]