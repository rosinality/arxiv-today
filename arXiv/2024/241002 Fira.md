https://arxiv.org/abs/2410.01623

*Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?* (Xi Chen, Kaituo Feng, Changsheng Li, Xunhao Lai, Xiangyu Yue, Ye Yuan, Guoren Wang)

> Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. This raises a question: whether it is possible to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes? In this paper, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to achieve this goal. First, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. Based on this observation, we propose a norm-based scaling method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to enable full-rank training. In this way, we can preserve the low-rank constraint in the optimizer while achieving full-rank training for better performance. Moreover, we find that there are sudden gradient rises during the optimization process, potentially causing loss spikes. To address this, we further put forward a norm-growth limiter to smooth the gradient via regulating the relative increase of gradient norms. Extensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore, achieving performance that is comparable to or even better than full-rank training.

Low Rank Training. GaLore에 (https://arxiv.org/abs/2403.03507) 더해 Low Rank 업데이트에 대한 Correction Term을 추가하고 싶은데 이 Correction Term에 대한 Adam Scaling을 구할 수가 없다, 그런데 Low Rank 학습과 Full Rank 학습에서 이 Scaling Factor가 비슷하기 때문에 이를 활용한다, 이런 아이디어입니다.

<english>
Low Rank Training. This study tried to correction term for low rank update to GaLore (https://arxiv.org/abs/2403.03507), but by default it is not possible to get adam scaling for this correction term. But as this scaling factor is similar between low and full rank training, we can utilize it.
</english>

#efficient-training

# Links

[[240306 GaLore.md]]