https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/

*SAM 2: Segment Anything in Images and Videos* (Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, Christoph Feichtenhofer)

> We present Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3× fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6× faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.

https://ai.meta.com/blog/segment-anything-2

SAM 2가 나왔군요. 비디오에 대한 확장입니다. 이미지 Feature와 출력 마스크에 대한 정보를 메모리에 저장하고 꺼내오는 방식입니다.

비디오에 대한 Interactive Segmentation이 이렇게 간명한 방식으로 풀렸다는 게 굉장하네요. 물론 데이터 구축에 투입한 노력이 그만큼 굉장하겠습니다만.

#video-segmentation 