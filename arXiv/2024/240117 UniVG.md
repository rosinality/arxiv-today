https://arxiv.org/abs/2401.09084

*UniVG: Towards UNIfied-modal Video Generation* (Ludan Ruan, Lei Tian, Chuanwei Huang, Xu Zhang, Xinyan Xiao)

> Diffusion based video generation has received extensive attention and achieved considerable success within both the academic and industrial communities. However, current efforts are mainly concentrated on single-objective or single-task video generation, such as generation driven by text, by image, or by a combination of text and image. This cannot fully meet the needs of real-world application scenarios, as users are likely to input images and text conditions in a flexible manner, either individually or in combination. To address this, we propose a Unified-modal Video Genearation system that is capable of handling multiple video generation tasks across text and image modalities. To this end, we revisit the various video generation tasks within our system from the perspective of generative freedom, and classify them into high-freedom and low-freedom video generation categories. For high-freedom video generation, we employ Multi-condition Cross Attention to generate videos that align with the semantics of the input images or text. For low-freedom video generation, we introduce Biased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the content of the input conditions. Our method achieves the lowest Fr\'echet Video Distance (FVD) on the public academic benchmark MSR-VTT, surpasses the current open-source methods in human evaluations, and is on par with the current close-source method Gen2. For more samples, visit https://univg-baidu.github.io.

이미지 혹은 텍스트 입력을 기반으로 생성하는 모델, 이 모델을 추가로 튜닝해 이미지에 정렬된 영상을 생성하는 모델, 그리고 그 위에 Super Resolution을 올렸군요. 영상은 시간축도 고려해야 해서 조건을 주입하는 것이 더 중요할 듯 한데 앞으로 어떻게 진행될지 궁금하네요.

#video_generation 