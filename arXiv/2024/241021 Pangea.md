https://arxiv.org/abs/2410.16153

*Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages* (Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig)

> Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.

기계 번역 기반 Multilingual Multimodal Instruction + 문화적 특성을 고려해 샘플링한 이미지에 대해 생성한 캡션과 Instruction으로 구성한 데이터셋과 벤치마크군요.

<english>
Dataset and benchmark consists of multilingual multimodal instruction based on machine translation, and generated captions and instructions on images sampled considering cultural charateristics.
</english>

#multimodal #multilingual #dataset