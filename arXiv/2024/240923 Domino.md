https://arxiv.org/abs/2409.15241

*Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping* (Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase)

> Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.

Tensor Parallel에서 Communication과 Computation의 중복을 최대화하려는 시도인데...Transformer의 레이어가 XAB 형태라고 한다면 X를 Row Sharding하고 B를 Column Sharding한 형태군요. A는 Sharding을 하지 않습니다. A를 Sharding하지 않는다는 것은 조금 아쉽지 않을까 싶네요.

#parallelism #efficient-training 