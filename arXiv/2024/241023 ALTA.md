https://arxiv.org/abs/2410.18077

*ALTA: Compiler-Based Analysis of Transformers* (Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, Kristina Toutanova)

> We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.

트랜스포머로 컴파일할 수 있는 언어. RASP와 다른 점은 Universal Transformer를 타겟해서 루프를 지원한다는 점이겠군요. 트랜스포머가 거의 유일한 아키텍처가 된 상황에서는 학습이 가능한가는 차치하더라도 트랜스포머의 표현력의 한계 자체도 중요한 정보겠죠.

<english>
Language that can be compiled to transformer weights. Difference to RASP is that it supports loop targeting universal transformers. As transformer is sole architecture for neural networks, besides of it is learnable or not, representation capacity of transformer itself became meaningful informations.
</english>

#transformer 