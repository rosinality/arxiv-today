https://arxiv.org/abs/2404.16811

*Make Your LLM Fully Utilize the Context* (Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou)

> While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.

텍스트의 일부를 잘라서 QA Insturction을 만드는 것은 보통 Long Context 튜닝에서 하는 작업과 비슷한데 이쪽은 여러 텍스트 세그먼트를 합쳐서 QA를 만드는 것과 함께 세그먼트들을 랜덤한 다른 세그먼트와 섞어서 Long Context를 만들었다는 것이 특징이군요. 이쪽이 다른 방식의 데이터셋 구축보다 나은지 궁금하긴 합니다.

#long-context 