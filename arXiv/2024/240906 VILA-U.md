https://arxiv.org/abs/2409.04429

*VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation* (Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu)

> VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.

이쪽도 Autoregressive Image Generation과 Recognition을 태클했군요. 다만 Vision Encoder에서 Image-Text Alignment와 Reconstruction을 모두 할 수 있게 하는 쪽에 방점이 찍혀 있습니다. 그냥은 안 되고 CLIP을 가져와서 Reconstruction을 추가 학습시키는 방향으로 해야 했다고 하네요.

Quantization에 RQ-VAE를 사용해서 여기에서도 깊이에 따라 순차적인 생성을 하게 됩니다.

#clip  #autoregressive-model #image-generation 