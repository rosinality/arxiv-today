https://arxiv.org/abs/2407.20177

*AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs* (Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia)

> To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. To address this challenge, we propose *AutoScale*, an automated tool that finds a compute-optimal data composition for training at any desired target scale. AutoScale first determines the optimal composition at a small scale using a novel bilevel optimization framework, Direct Data Optimization (*DDO*), and then fits a predictor to estimate the optimal composition at larger scales. The predictor's design is inspired by our theoretical analysis of scaling laws related to data composition, which could be of independent interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2 Large) on RedPajama dataset, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks. On pre-training Encoder-only LMs (BERT) with masked language modeling, DDO is shown to decrease loss on all domains while visibly improving average task performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by 5.9% compared with without reweighting. AutoScale speeds up training by up to 28%. Our codes are open-sourced.

데이터셋의 비율을 결정하기 위한 방법. 기본적인 아이디어는 Scaling Law에서 각 데이터셋의 학습량 N의 차이를 둔 두 개의 학습 결과를 비교해서 데이터셋을 사용하지 않은 경우의 Loss와 (즉 다른 데이터셋이 이 데이터셋의 Loss 감소에 준 영향) 데이터셋을 학습시켰을 때의 Loss를 추정하는 방법을 기반으로 합니다. 이를 통해 타겟 Objective인 모든 Validation 셋에 대한 Loss의 합을 최적화 하는 방법으로 데이터셋 비율을 결정합니다.

#dataset #scaling-law