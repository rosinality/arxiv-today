https://arxiv.org/abs/2404.19756

*KAN: Kolmogorov-Arnold Networks* (Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljačić, Thomas Y. Hou, Max Tegmark)

> Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.

Kolmogorov-Arnold representation theorem (https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem), 즉 임의의 다변수 연속함수를 Univariate 연속함수의 유한합으로 표현할 수 있다는 정리에 근거해 모델을 구성했습니다.

Kolmogorov Arnold 정리의 문제는 이 임의의 함수에 대해 Univariate 함수가 Smooth하다고 보장할 수 없다는 것이었습니다. 그런데 그에 대해 Smooth한 함수로 할 수 있는 만큼 하면 안 될까? 하는 아이디어네요.

여기서는 함수로 B-Spline을 사용했습니다. 결과적으로는 각 Weight마다 B-Spline 계수가 들어가는 형태라 대규모에서 실용적이기는 어려울 듯 합니다만 Scaling 등의 측면에서 흥미로운 결과가 나왔네요. 또한 이 Spline을 사용해 일종의 Symbolic Regression 문제에 접근하기도 했습니다.

이런 연구는 오랜만이라 재미있네요.

#mlp 