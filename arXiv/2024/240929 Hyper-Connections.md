https://arxiv.org/abs/2409.19606

*Hyper-Connections* (Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, Xun Zhou)

> We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.

여러 개의 Residual Stream을 놓고 이 Stream들을 어떻게 업데이트 할 것인지를 학습시킨 시도라고 할 수 있겠네요. ResNet 시절의 추억에 젖게 하는 연구군요. (https://arxiv.org/abs/1904.01569)

<english>
Using multiple residual streams and learn how to update each streams. It reminds me the era of ResNets. 
</english>(https://arxiv.org/abs/1904.01569)

#transformer

# Links

