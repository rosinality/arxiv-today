https://arxiv.org/abs/2406.16768

*WARP: On the Benefits of Weight Averaged Rewarded Policies* (Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, Olivier Bachem)

> Reinforcement learning from human feedback (RLHF) aligns large language models (LLMs) by encouraging their generations to have high rewards, using a reward model trained on human preferences. To prevent the forgetting of pre-trained knowledge, RLHF usually incorporates a KL regularization; this forces the policy to remain close to its supervised fine-tuned initialization, though it hinders the reward optimization. To tackle the trade-off between KL and reward, in this paper we introduce a novel alignment strategy named Weight Averaged Rewarded Policies (WARP). WARP merges policies in the weight space at three distinct stages. First, it uses the exponential moving average of the policy as a dynamic anchor in the KL regularization. Second, it applies spherical interpolation to merge independently fine-tuned policies into a new enhanced one. Third, it linearly interpolates between this merged model and the initialization, to recover features from pre-training. This procedure is then applied iteratively, with each iteration's final model used as an advanced initialization for the next, progressively refining the KL-reward Pareto front, achieving superior rewards at fixed KL. Experiments with GEMMA policies validate that WARP improves their quality and alignment, outperforming other open-source LLMs.

Policy의 EMA에 대한 KL Penalty를 걸고 학습을 진행, 학습한 여러 모델을 Spherical Interpolation으로 머지, 머지된 모델과 초기 모델의 Linear Interpolation으로 새로운 Inital Policy를 생성, 그리고 반복이라는 형태의 Alignment 알고리즘입니다. 전반적으로 여러 방향으로 보다 빠르게 탐색하고, 탐색 결과를 통합하고, 탐색 결과가 초기 모델에서 너무 멀어지지 않도록 조정한 다음 다시 탐색한다는 느낌이네요.

#alignment #rlhf 