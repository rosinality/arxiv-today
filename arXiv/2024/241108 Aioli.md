https://arxiv.org/abs/2411.05735

*Aioli: A Unified Optimization Framework for Language Model Data Mixing* (Mayee F. Chen, Michael Y. Hu, Nicholas Lourie, Kyunghyun Cho, Christopher Ré)

> Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code, math). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting regression models over training runs to dynamically updating proportions throughout training. Surprisingly, we find that no existing method consistently outperforms a simple stratified sampling baseline in terms of average test perplexity per group. In this paper, we study the cause of this inconsistency by unifying existing methods into a standard optimization framework. We show that all methods set proportions to minimize total loss, subject to a method-specific mixing law -- an assumption on how loss is a function of mixture proportions. We find that existing parameterizations of mixing laws can express the true loss-proportion relationship empirically, but the methods themselves often set the mixing law parameters inaccurately, resulting in poor and inconsistent performance. Finally, we leverage the insights from our framework to derive a new online method named Aioli, which directly estimates the mixing law parameters throughout training and uses them to dynamically adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out of 6 datasets by an average of 0.28 test perplexity points, whereas existing methods fail to consistently beat stratified sampling, doing up to 6.9 points worse. Moreover, in a practical setting where proportions are learned on shorter runs due to computational constraints, Aioli can dynamically adjust these proportions over the full training run, consistently improving performance over existing methods by up to 12.01 test perplexity points.

온라인으로 학습 분포를 조절하는 방법에 대한 연구. DoReMi 같은 기존 방법들을 통합해서 관찰했을 때 서로 다른 데이터셋이 다른 데이터셋의 Loss에 미치는 영향을 나타내는 행렬 A에 대한 추정이 부정확하다고 하네요. 이 부분을 개선한 알고리즘입니다.

<english>
A method for online adjustment on the ratio of datasets. After analyzing previous methods like DoReMi in unified perspective, estimation of matrix A which encodes the influence of the dataset on the other dataset is inaccurate. Suggested algorithm improves this.
</english>

#dataset #efficient-training 