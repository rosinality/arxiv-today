https://blog.google/technology/developers/gemma-open-models/
https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf

구글이 2B/7B 모델을 공개했군요. Gemini 기반이라는데 사실 Gemini와 얼마나 비슷할지는 모르겠습니다만 흥미로운 점들이 있습니다.

일단 MQA (2B), RoPE를 사용하고 있고 놀랍게도(?) GeGLU를 썼습니다. 그리고 RMSNorm을 레이어 입력과 출력에 모두 사용하는 형태군요. 레이어 숫자는 Llama에 비해 작은데 FFN이 임베딩 차원의 16배(!) 입니다. 헤드 차원도 보통의 2배인 256 차원이고 헤드 수는 그에 따라 절반이네요. 이 모델을 2B는 2T, 6B는 6T 토큰에 대해 학습시켰습니다.

Multimodal이 아닌 것은 그렇다 치고 Multilingual에 대한 고려보다는 영어 위주라는 것은 아쉬운 부분이네요. 토크나이저도 Gemini의 서브셋이라고 하는데 256K Vocabulary라는 것으로 봐선 그래도 비영어권 문자에 대한 효율성은 좀 낫지 않을까 싶습니다.

벤치마크 스코어는 Mistral 7B를 상회하는 수준입니다. 사실 Mistral 7B를 넘어 Mixtral 쪽이 대세가 되고 있는 것 같긴 한데 여하간 7B 모델에 얼마나 눌러 넣을 수 있는지를 보여주는 사례가 될 듯 하네요.