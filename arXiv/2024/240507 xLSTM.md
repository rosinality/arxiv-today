https://arxiv.org/abs/2405.04517

*xLSTM: Extended Long Short-Term Memory* (Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter)

> In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.

xLSTM이 Hype를 뚫고 정말로 등장했군요. LSTM에서 Sigmoid 게이트를 지수함수로 교체한 sLSTM과 요즘 Linear RNN과 비슷한 스타일의 Matrix state를 갖는 mLSTM 두 가지를 섞었네요.

Linear RNN과 Attention을 섞는 최근 하이브리드 모델과는 다르게 Linear RNN과 보다 전통적인 RNN의 조합이군요. RNN의 효과를 보려면 이 전통적인 RNN이 필요하다는 이야기가 나왔었죠. (https://arxiv.org/abs/2404.08819) 이 논문의 결과에서도 그런 느낌입니다.

#state-space-model

# Links

[[240412 The Illusion of State in State-Space Models.md]]