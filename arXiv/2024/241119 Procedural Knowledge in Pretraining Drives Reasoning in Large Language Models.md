https://arxiv.org/abs/2411.12580

*Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models* (Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, Max Bartolo)

> The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.

Influence function을 사용해 LLM이 지식과 추론 과제에 대해 응답할 때 어떤 문서에서 영향을 받았는지 분석했군요. 결과가 꽤 재미있습니다. 추론 과제에 대해서는 구체적인 정답이 등장한 문서가 아니라 추론 방법, 즉 절차적 지식이 포함된 문서에 영향을 받고, 따라서 같은 추론 과제에 대해서는 영향을 받은 문서들이 비슷한 경향이 있습니다. (높은 상관관계) 반대로 지식을 묻는 경우엔 정답이 등장한 구체적인 문서에 영향을 받는군요. 그리고 추론 과제에서는 중요한 문서들이 수학, StackExchange, arXiv, 코드였다는 것까지. Anthropic이 Influence function을 사용해 분석했을 때처럼 (https://arxiv.org/abs/2308.03296) LLM이 도메인을 넘나드는 정보를 활용할 수 있다는 사례겠네요.

<english>
Analyzing LLM is got influence from which document when make an answer for knowledge or reasoning tasks, using influence function. The result is quite interesting. For reasoning tasks intead of the document which contains exact solutions, LLM is influenced by documents that has procedural knowledges, thus for same reasoning tasks influential documents were similar each other (high correlation) Conversly, for knowledge tasks the model got an influence from specific documents which contain the answer. And for reasoning tasks influential documents are from mathematics, StackExchange, arXiv, and code. Similar to previous study from Anthropic which also employed influence function, (https://arxiv.org/abs/2308.03296) this study shows that LLM can employ cross domain information for the task.
</english>

#mechanistic-interpretation #reasoning

# Links

