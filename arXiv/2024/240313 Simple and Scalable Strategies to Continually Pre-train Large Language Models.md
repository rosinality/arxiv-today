https://arxiv.org/abs/2403.08763

*Simple and Scalable Strategies to Continually Pre-train Large Language Models* (Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish)

> Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.

Continual Pretraining에 대한 세팅 탐색. 이전 트레이닝 스케줄 때와 같은 수준의 LR로 LR Warmup, LR Decay, 이전 트레이닝 데이터를 5% 정도 섞기가 발견한 레시피군요. Infinite LR 스케줄이 편리하지 않을까 하는 이야기도 하고 있습니다. 추가 프리트레이닝을 해야할 경우가 빈번하다는 경험 때문인지 중국 쪽에서는 요새 Infinite LR 스케줄이 인기 있죠.

#continual-learning 