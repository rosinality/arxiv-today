https://arxiv.org/abs/2412.14711

*ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing* (Ziteng Wang, Jianfei Chen, Jun Zhu)

> Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.

MoE의 라우터를 Top-K 대신 ReLU로 교체. 물론 ReLU는 활성화되는 Expert의 수를 통제할 수 없으니 L1 regularization을 사용해 Sparsity를 유도합니다. 아이러니하게도 Sparse Autoencoder에서 L1 regularization에서 Top-K로 나아간 것의 반대 방향이네요. (https://arxiv.org/abs/2406.04093)

<english>
Replacing routher of MoE to ReLU instead of Top-K. As it is not possible to control the number of activated experts they induce sparsity using L1 regularization. Ironically, it is reverse direction that the case of sparse autoencoder which advanced to Top-K from L1 regularization. (https://arxiv.org/abs/2406.04093)
</english>

#moe

# Links

[[240606 Scaling and evaluating sparse autoencoders.md]]