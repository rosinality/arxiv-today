https://arxiv.org/abs/2408.15240

*Generative Verifiers: Reward Modeling as Next-Token Prediction* (Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal)

> Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time compute via majority voting for better verification. We demonstrate that when using Gemma-based verifiers on algorithmic and grade-school math reasoning tasks, GenRM outperforms discriminative verifiers and LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems solved with Best-of-N. Furthermore, we show that GenRM scales favorably across dataset size, model capacity, and inference-time compute.

Verifier 학습에 Verification에 더해 정답을 생성하도록 학습하면 Verification 성능이 높아진다는 결과. 추가적으로 Verification에 CoT를 하도록 학습할 수도 있습니다. 반대로 Generator도 Verification을 하도록 학습시키면 성능이 향상된다고 하네요.

얼마 전 Critique을 생성하도록 학습시킨 Reward Model이 나왔었죠. (https://arxiv.org/abs/2408.11791) 이쪽은 Verifier이고 Critique이 아니라 정답 생성이지만 비슷한 점은 있는 듯 하네요.

#search

# Links

[[240821 Critique-out-Loud Reward Models.md]]