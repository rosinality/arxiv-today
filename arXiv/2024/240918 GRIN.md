https://arxiv.org/abs/2409.12136

*GRIN: GRadient-INformed MoE* (Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen)

> Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16$\times$3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.

SparseMixer를 (https://arxiv.org/abs/2310.00811) 개선해서 Autoregressive LM에 적용해봤군요. SparseMixer는 간단하게는 Straight-Through Estimator나 REINFORCE를 적용하는 것처럼 MoE 라우터에 대한 그래디언트를 추정하기 위한 방법입니다. 좀 더 큰 규모에서 잘 작동할지 궁금했었는데 큰 문제가 없는 것 같군요.

Sparse MoE는 본질적으로 가장 중요한 부분이라고 할 수 있는 라우터 학습에 대해 Workaround를 적용할 수밖에 없죠. 이 부분을 어떻게 개선하는가에 따라 잠재적으로 상당히 유의미한 개선을 기대할 수 있지 않을까 싶습니다. 최근 이런 시도들이 나오고 있는데 (https://arxiv.org/abs/2408.15664) 실험해보면 재미있을 것 같네요.

#moe

# Links

[[240828 Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts.md]]
[[231001 Sparse Backpropagation for MoE Training.md]]