https://openai.com/research/video-generation-models-as-world-simulators

Sora의 테크니컬 리포트가 나왔네요.

Unreal 5나 NeRF 같은 도구로 합성한 데이터가 들어갔으리라는 추측이 많았지만 그런 흔적은 없습니다. 언급하지 않는다는 것이 부재한다는 증명은 아니지만요.

대신 보이는 것은 Spatiotemporal Latent에 대한 Diffusion Transformer 뿐이군요. 그리고 그 Diffusion Transformer를 크롭하지도 리사이즈 하지도 않은 비디오로 학습시켰습니다. 그리고 이를 뒷받침하는 충분한 연산력 투입. 결과적으로 최대 1920x1080p 영상에 대한 샘플링이 가능하군요. (2048x2048 이미지 샘플링도 가능하다고 합니다.) 추가적으로 DALL-E 3처럼 Recaptioning을 했습니다.

그리고 이렇게 학습된 모델에서 3D에 대한 어떠한 Inductive Bias도 없이 3D 일관성 장면의 연속성, 대상 영속성이 창발적으로 나타났습니다. 지금 이 시점에 규모의 힘을 의심해서는 안 되죠.

비디오 모델이 World Model이 될 수 있다는 추측이 많이 있었는데 비디오 내에서 행위에 의한 세상의 변화 과정을 시뮬레이션 하는 패턴이 나타났다는 것이 그걸 강력하게 뒷받침하고 있는 것으로 보입니다.

더 많은 데이터, 더 거대한 모델, 더 막대한 연산력이 문제를 해결한다는 것을 다시 보여주는군요.

#video-generation #world-models

Technical report of Sora is out.

There are guess that synthetic data generated by Unreal 5 or NeRF is used, but there are no hints about that. The fact that they did not mention is not the evidence for that it is not used, though.

Instead only (rather plain) Diffusion Transformer on spatiotemporal latents is proposed. Then that diffusion transformer is trained on video, without cropping or resizing. And enough computational power is used for training it. Consequently it is possible to sample 1920x1080p videos. (Also it can sample 2048x2048 static images.) Additionally they did Re-captioning like in DALLE-3.

Then trained model shows emergent phenomena like 3d consistency, scene coherence, object permanance without any 3d inductive biases. Why you have a doubt about the power of scales?

There are many thoughts around video models could be potential world models. This model shows that it can simulate effect of actions on world, and this powerfully supports that idea.

More data, Larger model, Powerful computational power solves the problem. It is proved again.