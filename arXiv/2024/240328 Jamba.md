https://arxiv.org/abs/2403.19887

*Jamba: A Hybrid Transformer-Mamba Language Model* (Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham)

> We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.

Jamba (https://www.ai21.com/blog/announcing-jamba) 리포트가 나왔군요. 전반적으로 Mixtral과 비슷하다는 느낌이 드는데 Mixtral 수준의 모델을 만드는 것이 쉽지 않다는 것을 고려하면 그 자체로 흥미롭지 않나 싶네요.

Long Context 과제들에 대해서도 성능이 나쁘지 않는 것 같습니다. 흥미롭네요. 공개된 모델이니 분석해보면 여러모로 SSM/Attention 하이브리드의 가능성에 대해 알 수 있지 않을까 싶습니다.

#state-space-model #transformer #llm 