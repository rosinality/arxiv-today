https://arxiv.org/abs/2403.20327

*Gecko: Versatile Text Embeddings Distilled from Large Language Models* (Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim)

> We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.

합성 데이터를 사용한 임베딩 모델 학습. 일단 QA 데이터셋의 Question-Answer 페어 등을 사용해 사전 파인튜닝을 합니다. 그리고 합성 데이터를 사용하는데 순서는 다음과 같습니다.

1. 웹 코퍼스에서 문서를 가져와 그 문서에 대해 수행할 과제를 작성하고 작성한 과제 기반으로 질문을 생성
2. 임베딩 모델을 사용해 질문에 대한 Positive Candidate를 Retrieval하고 LLM을 사용해 랭킹함. 마찬가지로 Hard Negative Candidate도 수집

1번 단계로 다양성을 확보하고 2번 단계로 퀄리티를 확보한다고 할 수도 있겠습니다.

#synthetic-data #retrieval 