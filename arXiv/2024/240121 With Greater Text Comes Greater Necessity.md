https://arxiv.org/abs/2401.11504

*With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation* (Y. Wang, D. Ma, D. Cai)

> Long text generation, such as novel writing or discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, Temp-Lora embeds this information directly into the model's parameters. In the process of long text generation, we use a temporary Lora module, progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling benchmark and the GuoFeng discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long texts, as indicated by a 13.2% decrease in perplexity on a subset of PG19, and a 29.6% decrease in perplexity along with a 53.2% increase in BLEU score on GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. While ensuring a slight improvement in generation quality (a decrease of 3.8% in PPL), it enables a reduction of 70.5% in the FLOPs required for inference and a 51.5% decrease in latency.

Dynamic Evaluation에 대해 생각해보면서 모델 자체에 컨텍스트를 입력하는 방법에 대해 생각해봤었는데 그런 시도가 나왔네요. 생성해나가면서 생성 결과를 사용해 LoRA를 업데이트하는 방식입니다. 평가 세팅을 정확하게 이해하기 어렵긴 한데 코드를 기다려봐야 할 것 같네요.

#long-context #continual-learning
