https://arxiv.org/abs/2402.06457

*V-STaR: Training Verifiers for Self-Taught Reasoners* (Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal)

> Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.

샘플링 -> 레이블링 -> 정답만 모아 다시 SFT라는 루프에서 오답을 따로 모은 다음 정답과 오답으로 DPO를 해서 Verifier를 추가로 학습하는 방법. 그리고 테스트 시점에서 Verifier를 사용해서 샘플을 필터링합니다. 학습 도메인에 대한 성능 향상과 GSM8K -> MATH, MBPP -> HumanEval로의 Transfer을 보여주고 있네요.

#synthetic-data 

In addition to self improvement loop of sampling -> labeling -> SFT using correct samples, gather incorrect samples and doing DPO using correct and incorrect samples to make verifiers. Then in the test time sample is filtered using verifiers. This study shows performance improvement on training domains, as well as transfer of performances between GSM8K -> MATH, MBPP -> HumanEval.