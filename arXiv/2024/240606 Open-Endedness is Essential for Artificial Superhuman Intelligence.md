https://arxiv.org/abs/2406.04268

*Open-Endedness is Essential for Artificial Superhuman Intelligence* (Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, Tim Rocktaschel)

> In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data. Nevertheless, the creation of openended, ever self-improving AI remains elusive. In this position paper, we argue that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries. We conclude by examining the safety implications of generally-capable openended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.

Artificial Super Intelligence라는 단어가 언급될 수 있다는 것 자체가 놀랍네요.

여기서 ASI의 특징으로 제안하는 것은 Open-endedness인데 시스템의 Open-endedness란 시스템이 생성하는 것들이 Novelty와 Learnablity를 갖고 있다는 것입니다. Novelty는 관측자의 입장에서 생성 결과가 점점 더 예측하기 어려워진다는 것, Learnability는 생성 결과들을 더 많이 관측한 후에는 생성 결과를 예측할 수 있게 된다는 것입니다.

알파고의 예를 들면 알파고는 사람이 예측하기 어려운 기보를 생성해내지만, 동시에 사람이 알파고의 기보를 통해 무언가를 배울 수 있죠. 그렇지만 알파고는 기보를 학습한 사람도 예측하기 어려운 기보를 다시 생성해냅니다. 이것이 ASI의 특징이라고 이야기합니다.

https://x.com/fortnow/status/1797976663914848373

그런데 전 이런 상황도 재미있을 것 같네요. AI가 P vs NP에 대한 증명을 제공했고 Proof Assistant가 검증했지만 사람은 이해할 수 없다면? 같은 문제죠.

#position