https://arxiv.org/abs/2404.14396

*SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation* (Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan)

> The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data. In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation. We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks. Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. The models, codes, and datasets will be released in https://github.com/AILab-CVC/SEED-X.

이미지 입출력이 가능한 Vision Language 모델. Condition 입력이 가능한 ViT 인코더/디코더를 사용하고 이미지 생성 쿼리 입력이 들어온 경우 이 이미지 Feature를 예측하게 하는 방식이군요. 그 외에는 요즘 정석적인 Global 이미지와 크롭 이미지를 입력으로 주고 Detection Objective를 추가했습니다.

#vision-language #image-generation 
