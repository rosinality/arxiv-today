https://arxiv.org/abs/2409.12122

*Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement* (An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang)

> In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems.

Qwen 2.5 Math. DeepSeekMath 스타일의 (https://arxiv.org/abs/2402.03300) Common Crawl을 통한 추가 데이터 수집, 그리고 이 추가 데이터를 활용해 Qwen 2 72B 모델로 QA 데이터 생성. 이렇게 구축한 700B 데이터로 학습한 모델로 새로운 데이터를 추가 생성하고 데이터를 더 수집해 1T 데이터를 구축했습니다.

네, 수학 관련 데이터만 1T네요.

SFT와 Reward Model 학습용으로는 답이나 파이썬 인터프리터를 사용한 Rejection Sampling을 사용했군요.

#math #llm

# Links

[[240205 DeepSeekMath.md]]