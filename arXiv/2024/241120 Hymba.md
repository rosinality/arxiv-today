https://arxiv.org/abs/2411.13676

*Hymba: A Hybrid-head Architecture for Small Language Models* (Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Lin, Jan Kautz, Pavlo Molchanov)

> We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.

Mamba와 Self Attention을 병렬로 붙였군요. 아이디어는 특정한 위치의 레이어에서 일어나야 할 정보 처리에 적합한 것이 Mamba인지 Self Attention인지를 구분해야 한다는 문제를 해소한다는 것. 추가적으로 메타 토큰이라는 이름으로 Attention Sink/레지스터 토큰을 사용했군요. 거기에 Global/Sliding window Attention과 KV 캐시 공유까지.

하이브리드 모델들은 이미 꽤 괜찮은 것이 아닌가 싶습니다.

<english>
Concatenating Mamba and self attention in parallel. The idea is that it would solve the problem of assign appropriate layer (Mamba or self attention) for information processing would occur in specific layers. Additionally they employed attention sink (or register tokens) as a name of meta tokens, and also global/sliding window attention and KV cache sharing.

I think hybrid models are already quite competitive.
</english>

#state-space-model #transformer 