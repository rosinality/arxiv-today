https://arxiv.org/abs/2410.01131

*nGPT: Normalized Transformer with Representation Learning on the Hypersphere* (Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg)

> We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.

Transformer Residual Block을 Norm(x + a(Norm(x) - x))과 같은 형태로 설정. 과거와 최근의 Post Norm의 결합과 비슷한 형태군요. 거기에 Weight들을 매 학습 스텝마다 Normalize 해주는 것과 Scaling Factor들이 추가됐습니다. 이는 EDM2가 생각나게 하네요. (https://arxiv.org/abs/2312.02696)

과거의 Post Norm을 생각하면 깊이에 따른 불안정성이 있는데 이 세팅에서는 어떨지 모르겠네요. Layer Norm의 위치와 관련된 이전 연구들을 참조하는 것이 좋을지도 모르겠습니다.

<english>
Set residual blocks of transformer to be the form of Norm(x + a(Norm(x) - x)). It is similar to combining past and recent Post Norm into one blocks. In addition to that weight is normalized at every training steps and scaling factor is added. It reminds me EDM2. (https://arxiv.org/abs/2312.02696)

There was training instabilities with increase of depths in past Post Norm, but maybe it could be different in this settings. Maybe it is worth to refer previous researches on position of layer normalizations.
</english>

#transformer #normalization 

Set residual blocks of transformer of it is ssimilar to combining pasts and recent post norm into one blocks. in addition to that weight is normalized at every training stepss it reminds me EDM2

# Links

[[231205 Analyzing and Improving the Training Dynamics of Diffusion Models.md]]