https://arxiv.org/abs/2410.14672

*BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities* (Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, Kwan-Yee K. Wong)

> We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field.

Lookup-Free Quantization 같은 이진 양자화에 Masked Image Modeling을 결합한 실험. Autoregressive Model 혹은 이진 코드를 카테고리로 예측하는 방법들과의 비교가 있어 흥미롭습니다.

<english>
The experiment that combining binary quantization like lookup-free quantization and masked image modeling. What is interesting is that there are comparisons to autoregressive models or categorical prediction of binary codes.
</english>

#vq #image-generation 