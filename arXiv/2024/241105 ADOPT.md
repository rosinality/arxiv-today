https://arxiv.org/abs/2411.02853

*ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate* (Shohei Taniguchi, Keno Harada, Gouki Minegishi, Yuta Oshima, Seong Cheol Jeong, Go Nagahara, Tomoshi Iiyama, Masahiro Suzuki, Yusuke Iwasawa, Yutaka Matsuo)

> Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\beta_2$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\mathcal{O} ( 1 / \sqrt{T} )$ with any choice of $\beta_2$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt.

Adam의 수렴이 β_2에 영향을 받는 문제에 대한 수정. 방법은 간단하게 파라미터 업데이트 이후 모멘텀을 업데이트하고 Second moment를 사용한 Normalization을 모멘텀 업데이트로 이동시키는 것이네요. LaProp과 비슷하군요. (https://arxiv.org/abs/2002.04839)

테스트해본 사람들 사이에서는 괜찮은 것 같다는 평이 있네요. (https://x.com/wightmanr/status/1854930826279940579)

<english>
Fix of the problem that β_2 influences on convergence of Adam. The solution is simply updating momentum after the parameter updates and normalization using second moment is moved into momentum updates. It is similar to LaProp. (https://arxiv.org/abs/2002.04839)

There are comments among people tried to use it says that this works. (https://x.com/wightmanr/status/1854930826279940579)
</english>

#optimizer

# Links

