https://arxiv.org/abs/2406.07522

*Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling* (Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen)

> Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.

Mamba + Sliding Window Attention. Griffin (https://arxiv.org/abs/2402.19427) 과 비슷한 세팅인데 Attention이 1:1로 들어가는군요. 은근슬쩍 Phi3의 3.2T 토큰으로 학습시킨 결과를 가져왔군요.

Griffin에서도 나타났지만 Sliding Window Attention과의 조합이 Associative Recall 과제에서 문제가 생기는 듯 한데 그 부분에 대한 실험이 없다는 것은 조금 아쉽네요.

#state-space-model

# Links

