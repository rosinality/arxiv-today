https://shengdinghu.notion.site/MiniCPM-Unveiling-the-Potential-of-End-side-Large-Language-Models-d4d3a8c426424654a4e80e42a711cb20

MiniCPM이라는 2B 모델이 나오면서 테크니컬 리포트 비슷한 문서가 따라나왔는데 재미있네요.

중국 쪽 모델에서 자주 등장하는 레시피들이 있는데 그게 좀 더 구체적으로 나타났습니다. 배치 크기에 대한 Scaling Law 등을 사용해 4M 이상의 배치 크기로 확대하는 것 (https://arxiv.org/abs/2401.02954, https://arxiv.org/abs/2310.19341) 프리트레이닝을 여러 단계로 나누는 것 (https://raw.githubusercontent.com/InternLM/InternLM-techreport/main/InternLM.pdf) 프리트레이닝 데이터에 Instruction 데이터를 추가하는 것 (https://arxiv.org/abs/2309.16609) Learning Rate 스케쥴 자체를 여러 단계의 프리트레이닝과 Continual Pretraining에 용이하게 설정하는 것 (https://arxiv.org/abs/2401.02954) 등이 있습니다.

좀 더 구체적으로는 μP에 영향을 받아 레이어 출력에 대한 특정한 Scaling Factor를 사용했고 (https://arxiv.org/abs/2203.03466, https://arxiv.org/abs/2304.03208) 과 확연히 높은 Learning Rate, 그리고 Learning Rate 스케쥴에 Warmup-Plateau-Decay 스타일, 즉 LR을 높이고-LR을 높은 상태로 유지시켜서 학습하고-LR을 감소시키는 방법을 사용했다고 이야기하고 있습니다. Warmup-Plateau-Decay는 꽤 고전적인 방법인데 사전에 학습 스텝 수를 결정해야 한다는 Cosine 스케쥴에 대한 대안으로 등장하고 있는 것으로 보이네요. (https://arxiv.org/abs/2106.04560)

그리고 학습 후반에 Instruction과 높은 퀄리티의 데이터를 학습시킨다는, 사람들이 종종 높은 성능의 LLM 학습의 레시피가 아니었을까 의심했던 방법이 들어가 있습니다.

이렇게 해서 파라미터-데이터의 비율이 1:20 정도였던 Chinchilla Optimal에 비해 1:100까지 높아졌다는 이야기도 하고 있습니다. 물론 최근의 작은 모델들은 이미 Chinchilla Optimal을 훨씬 뛰어넘는 양의 데이터에 대해 학습되고 있었지만요. (Chinchilla Optimal에 따르면 10B 모델의 최적 학습 토큰 수는 200B 정도죠.)

#llm #pretraining 