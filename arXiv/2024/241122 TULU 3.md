https://allenai.org/papers/tulu-3-report.pdf

*TÜLU 3: Pushing Frontiers in Open Language Model Post-Training* (Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi)

> Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce TÜLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÜLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With TÜLU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the TÜLU 3 model weights and demo, we release the complete recipe – including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the TÜLU 3 approach to more domains.

TULU 3가 나왔군요. 새삼스럽지만 지금 시점의 포스트트레이닝은 굉장히 다방면에 대해서 직접 태클해나가야 하는 작업이 되었다는 느낌이 드네요. 프롬프트 큐레이션 등이 흥미롭습니다.

이미 정렬된 모델을 통해 생성한 데이터로 정렬하는 것을 그리 선호하지는 않지만 이런 형태의 데이터 구축 방법은 모델을 부트스트랩할 때에도 여전히 사용될 수 있겠죠. 그런 의미에서는 계속 체크해야 하는 듯 싶네요.

<english>
TULU 3 came out. Not surprisingly, post training in these days requires a lot of tinkering for a variety of area. Prompt curating is particulary intriguing.

I don't prefer the method that aligns a model with the data generated from already aligned model, but these kind of data synthesis can be also useful when we want to bootstrap a model. So I think it is worth to follow for these approaches.
</english>

#alignment 