https://arxiv.org/abs/2410.17131

*Aligning Large Language Models via Self-Steering Optimization* (Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, Junyang Lin)

> Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. $SSO$ can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of $SSO$ with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, $SSO$ leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by $SSO$ significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.

규칙 기반 Alignment 방법인데 좋은 규칙에 대해서 생성된 응답과 나쁜 규칙에 대해 생성된 응답이 있을 때 좋은 규칙 프롬프트에 대해서는 좋은 규칙에 대해 생성된 응답을 선호하고, 나쁜 규칙 프롬프트에 대해서는 규칙 제시 없이 생성된 응답을 선호한다는 형태군요. 다른 프롬프트에 대한 응답을 가져와 섞는 방법이 생각나네요. (https://arxiv.org/abs/2409.13156)

<english>
Principle based alignment method. Consider response generated from good principles and bad principles. For good principle prompts response from good principle is preferred, and for bad principle prompts response generated without a principle is preferred. It reminds me the method that take response from another prompts and mix. (https://arxiv.org/abs/2409.13156)
</english>

#rlaif

# Links

[[240920 RRM.md]]