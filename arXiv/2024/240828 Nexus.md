https://arxiv.org/abs/2408.15901

*Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts* (Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, Ahmet Üstün)

> Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on "upcycling" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.

도메인 특화 후 MoE로 결합하는 방법 연구. (https://arxiv.org/abs/2408.08274) DeepSeekMoE처럼 Shared Expert를 두는 것과 이후 MoE 모델을 추가 도메인에 대해 다시 확장하는 것을 탐색했군요.

사실 DeepSeekMoE처럼 Shared Expert에 더해 더 작은 Expert를 더 많이 활용하는 것이 나을 수도 있겠죠. 이쪽에 대해서는 Qwen MoE에서 시도한 FFN을 쪼개는 방법을 생각해볼 수도 있겠네요. (https://qwenlm.github.io/blog/qwen-moe/)

#moe

# Links

[[240815 BAM! Just Like That.md]]