https://arxiv.org/abs/2409.13156

*RRM: Robust Reward Model Training Mitigates Reward Hacking* (Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh)

> Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%.

Robust Reward Model을 위한 데이터 Augmentation 방법. 기본적인 아이디어는 프롬프트에 영향을 받지 않고 생성된 응답에만 영향을 받는 요인은 Preference에 영향을 미쳐서는 안 된다는 것입니다. 이를 위해 데이터셋 내에서 응답들을 섞어서 Negative나 Tie를 만들어 추가하는 방식으로 접근했네요. 데이터셋 내의 응답을 Negative로 설정해서 학습하는 방법은 이전에도 있었던 것 같긴 합니다.

#reward-model 