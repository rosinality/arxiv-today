https://arxiv.org/abs/2510.08565

*NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints* (Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai)

> Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.

Native 학습된 Vision-Language 모델의 Scaling 거동 (이미지 인코더는 From Scratch 학습, LLM은 프리트레이닝된 Weight 사용). 이미지 인코더의 역할은 텍스트 토큰과 비슷한 연산 수준에서 LLM 디코더가 이미지 토큰을 처리할 수 있도록 하는 것일 것. 이러한 연산 수준의 정렬은 특히 작은 모델에서 더 유용할 듯.

<english>
Scaling behavior of vision-language models when natively trained (visual encoder trained from scratch, LLM weights pretrained). The role of vision encoder would be allow LLM decoder to process vision tokens with similar computational levels of text tokens. These kind of alignment would be important, especially for smaller decoders.
</english>

#multimodal 