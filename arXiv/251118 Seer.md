https://arxiv.org/abs/2511.14617

*Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning* (Ruoyu Qin, Weiran He, Weixiao Huang, Yangkun Zhang, Yikai Zhao, Bo Pang, Xinran Xu, Yingdi Shan, Yongwei Wu, Mingxing Zhang)

> Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

Synchronous RL을 위한 추론 스케줄러. 1. 추론 리퀘스트를 더 작은 청크 단위로 쪼갬 2. 그룹 내에 존재하는 샘플링 길이의 상관관계를 사용해 샘플 길이를 추정하고 이를 사용해 리퀘스트를 스케줄링 3. 최대 드래프트의 길이를 조절하는 Adaptive Speculative Decoding을 사용.

Inference scheduler for synchronous RL. 1. Split each request into smaller chunks 2. Estimate length of responses in groups using intra-group correlation of lengths and schedule requests using this 3. Adaptive speculative decoding that adjusts maximum draft length.

#efficiency #rl 